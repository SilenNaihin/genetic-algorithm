{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neuroevolution Intuitions\n",
        "\n",
        "**From Gradient Descent to Evolution-Based Learning**\n",
        "\n",
        "This notebook builds intuition for neuroevolution - training neural networks without gradients.\n",
        "We'll start from first principles and work our way up to NEAT and the Evolution Lab implementation.\n",
        "\n",
        "## Prerequisites\n",
        "- Familiar with gradient descent and backpropagation\n",
        "- Know transformers at a high level\n",
        "- Have heard of grid search, Bayesian optimization\n",
        "- Can guess GA pseudocode but details are fuzzy\n",
        "\n",
        "## What You'll Learn\n",
        "1. Why gradients don't always work\n",
        "2. How genetic algorithms search solution spaces\n",
        "3. How to evolve neural network weights\n",
        "4. The topology evolution problem and NEAT's solution\n",
        "5. How Evolution Lab implements these concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "from utils import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Callable\n",
        "from dataclasses import dataclass\n",
        "from copy import deepcopy\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8-whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. The Problem: Optimization Without Gradients\n",
        "\n",
        "You know gradient descent: compute $\\nabla_{\\theta} L$, step in the negative direction, repeat.\n",
        "It's the workhorse of deep learning. But it has a critical requirement:\n",
        "\n",
        "**The loss function must be differentiable with respect to the parameters.**\n",
        "\n",
        "This breaks down in many real-world scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Why Gradients Fail: A Physics Simulation Example\n",
        "\n",
        "Imagine you're training a neural network to control a robot in a physics simulation:\n",
        "\n",
        "```\n",
        "Neural Net --> Actions --> Physics Engine --> State --> Fitness\n",
        "   theta        a(t)        [BLACK BOX]       s(t)      f(s)\n",
        "```\n",
        "\n",
        "To use backprop, you need $\\frac{\\partial f}{\\partial \\theta}$. This requires:\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial s} \\cdot \\frac{\\partial s}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\theta}$$\n",
        "\n",
        "The problem? **Physics engines are non-differentiable black boxes.**\n",
        "\n",
        "- Collision detection: discrete events (contact or no contact)\n",
        "- Friction: discontinuous (static vs kinetic)\n",
        "- Rigid body constraints: inequality constraints\n",
        "\n",
        "Even if you had access to the source code, computing $\\frac{\\partial s}{\\partial a}$ would require\n",
        "differentiating through thousands of timesteps of chaotic dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create a simple non-differentiable fitness function\n",
        "# This simulates the kind of function we face in physics simulations\n",
        "\n",
        "def physics_fitness(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    A fitness function that's impossible to differentiate:\n",
        "    - Contains discontinuities (floor, sign)\n",
        "    - Has multiple local optima\n",
        "    - Simulates 'collision-like' behavior\n",
        "    \"\"\"\n",
        "    # Continuous part (we could differentiate this)\n",
        "    smooth = -((x - 2.5)**2)\n",
        "    \n",
        "    # Non-differentiable parts (these break backprop)\n",
        "    quantized = torch.floor(x * 3) / 3  # Discretization (like collision grid)\n",
        "    threshold = torch.where(x > 1.5, torch.ones_like(x), torch.zeros_like(x))  # Step function\n",
        "    \n",
        "    # Combine them\n",
        "    fitness = smooth + 0.5 * torch.sin(quantized * 10) + threshold\n",
        "    return fitness\n",
        "\n",
        "# Visualize it\n",
        "x = torch.linspace(0, 5, 500)\n",
        "y = physics_fitness(x)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(x.numpy(), y.numpy(), 'b-', linewidth=2)\n",
        "plt.xlabel('Parameter x')\n",
        "plt.ylabel('Fitness')\n",
        "plt.title('A Non-Differentiable Fitness Landscape\\n(Multiple optima, discontinuities, flat regions)')\n",
        "plt.axhline(y=y.max().item(), color='r', linestyle='--', alpha=0.5, label=f'Global max: {y.max().item():.2f}')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Try computing gradients through torch.floor() - PyTorch will give you zeros!\")\n",
        "x_test = torch.tensor([2.0], requires_grad=True)\n",
        "y_test = torch.floor(x_test)\n",
        "y_test.backward()\n",
        "print(f\"Gradient of floor(x) at x=2.0: {x_test.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Search Strategies Compared\n",
        "\n",
        "Without gradients, how do we find good solutions? Let's compare approaches.\n",
        "\n",
        "### Grid Search\n",
        "- Divide parameter space into a grid\n",
        "- Evaluate every point\n",
        "- Scales exponentially: $O(n^d)$ for $n$ points per dimension, $d$ dimensions\n",
        "\n",
        "### Random Search\n",
        "- Sample random points\n",
        "- Keep track of best\n",
        "- Better than grid for high dimensions (curse of dimensionality)\n",
        "\n",
        "### Population-Based Search (Genetic Algorithms)\n",
        "- Maintain a population of solutions\n",
        "- Good solutions reproduce, bad ones die\n",
        "- **Key insight**: information flows between solutions via crossover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def grid_search(fitness_fn: Callable, bounds: Tuple[float, float], n_points: int) -> Tuple[float, float]:\n",
        "    \"\"\"Grid search: evaluate fitness at evenly spaced points.\"\"\"\n",
        "    x_values = torch.linspace(bounds[0], bounds[1], n_points)\n",
        "    fitnesses = fitness_fn(x_values)\n",
        "    best_idx = fitnesses.argmax()\n",
        "    return x_values[best_idx].item(), fitnesses[best_idx].item()\n",
        "\n",
        "def random_search(fitness_fn: Callable, bounds: Tuple[float, float], n_samples: int) -> Tuple[float, float]:\n",
        "    \"\"\"Random search: sample random points uniformly.\"\"\"\n",
        "    x_values = torch.rand(n_samples) * (bounds[1] - bounds[0]) + bounds[0]\n",
        "    fitnesses = fitness_fn(x_values)\n",
        "    best_idx = fitnesses.argmax()\n",
        "    return x_values[best_idx].item(), fitnesses[best_idx].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare search strategies\n",
        "bounds = (0.0, 5.0)\n",
        "n_evals = 50  # Same budget for both\n",
        "\n",
        "# Run multiple trials of random search to see variance\n",
        "grid_result = grid_search(physics_fitness, bounds, n_evals)\n",
        "random_results = [random_search(physics_fitness, bounds, n_evals) for _ in range(10)]\n",
        "\n",
        "print(f\"Grid Search (n={n_evals}): x={grid_result[0]:.3f}, fitness={grid_result[1]:.3f}\")\n",
        "print(f\"Random Search (n={n_evals}, 10 trials):\")\n",
        "print(f\"  Best:  fitness={max(r[1] for r in random_results):.3f}\")\n",
        "print(f\"  Worst: fitness={min(r[1] for r in random_results):.3f}\")\n",
        "print(f\"  Mean:  fitness={sum(r[1] for r in random_results)/10:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 The Curse of Dimensionality\n",
        "\n",
        "With 1 dimension, grid search works fine. But neural networks have thousands to millions of parameters.\n",
        "\n",
        "Consider a tiny network with just 100 weights:\n",
        "- Grid search with 10 points per dimension: $10^{100}$ evaluations\n",
        "- That's more than atoms in the universe ($\\approx 10^{80}$)\n",
        "\n",
        "**We need smarter search strategies.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a 2D fitness landscape with multiple optima\n",
        "def multimodal_fitness_2d(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"A 2D fitne   ss landscape with multiple local optima.\"\"\"\n",
        "    # Global optimum near (2, 2)\n",
        "    f1 = 3 * torch.exp(-((x - 2)**2 + (y - 2)**2) / 0.5)\n",
        "    # Local optimum near (0, 0)\n",
        "    f2 = 2 * torch.exp(-((x)**2 + (y)**2) / 0.8)\n",
        "    # Local optimum near (4, 0)\n",
        "    f3 = 1.5 * torch.exp(-((x - 4)**2 + (y)**2) / 0.6)\n",
        "    # Ridges\n",
        "    f4 = 0.5 * torch.sin(x * 2) * torch.cos(y * 2)\n",
        "    return f1 + f2 + f3 + f4\n",
        "\n",
        "# Create grid for visualization\n",
        "xx = torch.linspace(-1, 5, 100)\n",
        "yy = torch.linspace(-1, 4, 100)\n",
        "X, Y = torch.meshgrid(xx, yy, indexing='ij')\n",
        "Z = multimodal_fitness_2d(X, Y)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Surface plot\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis', alpha=0.8)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('Fitness')\n",
        "ax1.set_title('3D Fitness Landscape')\n",
        "ax1.view_init(elev=30, azim=45)\n",
        "axes[0].remove()  # Remove the original 2D axis\n",
        "\n",
        "# Contour plot\n",
        "contour = axes[1].contourf(X.numpy(), Y.numpy(), Z.numpy(), levels=20, cmap='viridis')\n",
        "plt.colorbar(contour, ax=axes[1], label='Fitness')\n",
        "axes[1].set_xlabel('x')\n",
        "axes[1].set_ylabel('y')\n",
        "axes[1].set_title('Contour Plot - Multiple Optima')\n",
        "\n",
        "# Mark optima\n",
        "axes[1].plot(2, 2, 'r*', markersize=15, label='Global optimum')\n",
        "axes[1].plot(0, 0, 'wo', markersize=10, label='Local optimum')\n",
        "axes[1].plot(4, 0, 'wo', markersize=10)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key insight: Gradient descent starting from (4,0) would get stuck in the local optimum.\")\n",
        "print(\"Population-based methods explore multiple regions simultaneously.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. Genetic Algorithms from Scratch\n",
        "\n",
        "Genetic algorithms (GAs) are inspired by biological evolution:\n",
        "\n",
        "1. **Population**: Maintain multiple candidate solutions\n",
        "2. **Selection**: Fitter individuals are more likely to reproduce\n",
        "3. **Crossover**: Combine genes from two parents\n",
        "4. **Mutation**: Random changes to explore nearby solutions\n",
        "\n",
        "Let's build each component from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Population: Why Many Solutions at Once?\n",
        "\n",
        "**Point-based search** (gradient descent, hill climbing):\n",
        "- Maintains a single solution $\\theta$\n",
        "- Updates it iteratively: $\\theta_{t+1} = \\theta_t - \\eta \\nabla L$\n",
        "- Gets stuck in local optima\n",
        "\n",
        "**Population-based search** (genetic algorithms, evolution strategies):\n",
        "- Maintains $N$ solutions $\\{\\theta_1, \\theta_2, ..., \\theta_N\\}$\n",
        "- Solutions explore different regions simultaneously\n",
        "- Information flows between solutions via selection and crossover\n",
        "\n",
        "### The Key Insight\n",
        "\n",
        "A population is like running N parallel searches that can **share information**:\n",
        "- If solution A finds a good region, that information propagates to the next generation\n",
        "- If solution B finds a different good feature, crossover can combine A and B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "@dataclass\n",
        "class Individual:\n",
        "    \"\"\"A single candidate solution in our population.\"\"\"\n",
        "    genome: torch.Tensor  # The parameters we're optimizing\n",
        "    fitness: float = 0.0  # How good is this solution?\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Individual(genome={self.genome.tolist()}, fitness={self.fitness:.4f})\"\n",
        "\n",
        "\n",
        "def create_population(pop_size: int, genome_size: int, \n",
        "                       low: float = -5.0, high: float = 5.0) -> List[Individual]:\n",
        "    \"\"\"Create a random initial population.\"\"\"\n",
        "    population = []\n",
        "    for _ in range(pop_size):\n",
        "        genome = torch.rand(genome_size) * (high - low) + low\n",
        "        population.append(Individual(genome=genome))\n",
        "    return population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small population\n",
        "pop = create_population(pop_size=5, genome_size=2, low=0, high=5)\n",
        "\n",
        "print(\"Initial population (random genomes, fitness not yet evaluated):\")\n",
        "for i, ind in enumerate(pop):\n",
        "    print(f\"  {i}: {ind}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Selection: Survival of the Fittest\n",
        "\n",
        "Selection chooses which individuals get to reproduce. Better fitness = higher chance of being selected.\n",
        "\n",
        "### Tournament Selection\n",
        "1. Randomly pick $k$ individuals from the population\n",
        "2. Return the one with highest fitness\n",
        "3. Repeat to get multiple parents\n",
        "\n",
        "**Why tournament?**\n",
        "- Simple and effective\n",
        "- Selection pressure controlled by tournament size $k$\n",
        "- $k=2$: weak pressure (more exploration)\n",
        "- $k=N$: strong pressure (more exploitation, always picks the best)\n",
        "\n",
        "### Roulette Wheel Selection (Fitness Proportionate)\n",
        "1. Selection probability $\\propto$ fitness\n",
        "2. Like spinning a weighted wheel\n",
        "\n",
        "**Problems with roulette:**\n",
        "- Requires positive fitness values\n",
        "- Sensitive to fitness scaling (one very fit individual dominates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def tournament_selection(population: List[Individual], tournament_size: int = 3) -> Individual:\n",
        "    \"\"\"Select one individual using tournament selection.\"\"\"\n",
        "    # Randomly sample tournament_size individuals\n",
        "    tournament = random.sample(population, min(tournament_size, len(population)))\n",
        "    # Return the one with highest fitness\n",
        "    return max(tournament, key=lambda ind: ind.fitness)\n",
        "\n",
        "\n",
        "def roulette_selection(population: List[Individual]) -> Individual:\n",
        "    \"\"\"Select one individual using fitness-proportionate selection.\"\"\"\n",
        "    # Shift fitnesses to be positive\n",
        "    min_fitness = min(ind.fitness for ind in population)\n",
        "    shifted = [ind.fitness - min_fitness + 1e-6 for ind in population]\n",
        "    total = sum(shifted)\n",
        "    \n",
        "    # Spin the wheel\n",
        "    pick = random.random() * total\n",
        "    current = 0\n",
        "    for ind, fit in zip(population, shifted):\n",
        "        current += fit\n",
        "        if current >= pick:\n",
        "            return ind\n",
        "    return population[-1]  # Fallback\n",
        "\n",
        "\n",
        "import random  # Need this for the selection functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate selection\n",
        "# First, give our population some fitness values\n",
        "test_pop = create_population(5, 2, 0, 5)\n",
        "for i, ind in enumerate(test_pop):\n",
        "    ind.fitness = float(i)  # Fitness: 0, 1, 2, 3, 4\n",
        "\n",
        "print(\"Population with fitness values:\")\n",
        "for ind in test_pop:\n",
        "    print(f\"  {ind}\")\n",
        "\n",
        "# Run selection many times to see the distribution\n",
        "tournament_counts = {i: 0 for i in range(5)}\n",
        "roulette_counts = {i: 0 for i in range(5)}\n",
        "\n",
        "for _ in range(1000):\n",
        "    selected = tournament_selection(test_pop, tournament_size=2)\n",
        "    idx = test_pop.index(selected)\n",
        "    tournament_counts[idx] += 1\n",
        "    \n",
        "    selected = roulette_selection(test_pop)\n",
        "    idx = test_pop.index(selected)\n",
        "    roulette_counts[idx] += 1\n",
        "\n",
        "print(\"\\nSelection distribution (1000 trials):\")\n",
        "print(\"Fitness | Tournament(k=2) | Roulette\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(5):\n",
        "    print(f\"   {i}    |      {tournament_counts[i]:4d}       |   {roulette_counts[i]:4d}\")\n",
        "\n",
        "print(\"\\nNote: Higher fitness = selected more often\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Mutation: Exploring Nearby Solutions\n",
        "\n",
        "Mutation introduces random changes to a genome. This is how GAs **explore** the search space.\n",
        "\n",
        "### Gaussian Mutation\n",
        "For continuous parameters (like neural network weights):\n",
        "\n",
        "$$\\theta'_i = \\theta_i + \\mathcal{N}(0, \\sigma^2) \\quad \\text{with probability } p_{mut}$$\n",
        "\n",
        "**Key parameters:**\n",
        "- $p_{mut}$: probability each gene mutates (exploration rate)\n",
        "- $\\sigma$: mutation magnitude (step size)\n",
        "\n",
        "**Intuition:**\n",
        "- Small $\\sigma$: fine-tuning, local search\n",
        "- Large $\\sigma$: big jumps, escaping local optima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def mutate(individual: Individual, mutation_rate: float = 0.1, \n",
        "           mutation_magnitude: float = 0.3) -> Individual:\n",
        "    \"\"\"\n",
        "    Apply Gaussian mutation to an individual.\n",
        "    \n",
        "    Args:\n",
        "        individual: The individual to mutate\n",
        "        mutation_rate: Probability each gene mutates\n",
        "        mutation_magnitude: Standard deviation of Gaussian noise\n",
        "    \n",
        "    Returns:\n",
        "        A new mutated individual (original is not modified)\n",
        "    \"\"\"\n",
        "    new_genome = individual.genome.clone()\n",
        "    \n",
        "    # Create mutation mask (which genes to mutate)\n",
        "    mask = torch.rand(new_genome.shape) < mutation_rate\n",
        "    \n",
        "    # Apply Gaussian noise where mask is True\n",
        "    noise = torch.randn(new_genome.shape) * mutation_magnitude\n",
        "    new_genome = new_genome + mask.float() * noise\n",
        "    \n",
        "    return Individual(genome=new_genome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize mutation\n",
        "original = Individual(genome=torch.tensor([2.5, 2.5]))\n",
        "\n",
        "# Generate many mutations\n",
        "mutations_low_sigma = [mutate(original, mutation_rate=1.0, mutation_magnitude=0.2) for _ in range(200)]\n",
        "mutations_high_sigma = [mutate(original, mutation_rate=1.0, mutation_magnitude=1.0) for _ in range(200)]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for ax, mutations, sigma in [(axes[0], mutations_low_sigma, 0.2), \n",
        "                              (axes[1], mutations_high_sigma, 1.0)]:\n",
        "    xs = [m.genome[0].item() for m in mutations]\n",
        "    ys = [m.genome[1].item() for m in mutations]\n",
        "    \n",
        "    ax.scatter(xs, ys, alpha=0.5, s=20, label='Mutations')\n",
        "    ax.scatter([2.5], [2.5], c='red', s=100, marker='*', label='Original', zorder=5)\n",
        "    ax.set_xlim(0, 5)\n",
        "    ax.set_ylim(0, 5)\n",
        "    ax.set_xlabel('Gene 1')\n",
        "    ax.set_ylabel('Gene 2')\n",
        "    ax.set_title(f'Mutation Distribution (sigma={sigma})')\n",
        "    ax.legend()\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Small sigma: fine local search. Large sigma: broader exploration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Crossover: Combining Good Solutions\n",
        "\n",
        "Crossover creates offspring by combining genes from two parents.\n",
        "\n",
        "**Why crossover matters:**\n",
        "- Parent A might have good genes at positions [0, 1]\n",
        "- Parent B might have good genes at positions [2, 3]\n",
        "- Offspring could inherit the best of both!\n",
        "\n",
        "### Types of Crossover\n",
        "\n",
        "**Single-point crossover:**\n",
        "```\n",
        "Parent A: [a0, a1, | a2, a3]    -->   Child 1: [a0, a1, b2, b3]\n",
        "Parent B: [b0, b1, | b2, b3]    -->   Child 2: [b0, b1, a2, a3]\n",
        "```\n",
        "\n",
        "**Uniform crossover:**\n",
        "```\n",
        "Each gene independently chosen from either parent with 50% probability\n",
        "```\n",
        "\n",
        "**Blend crossover (for continuous):**\n",
        "```\n",
        "child[i] = alpha * parent_a[i] + (1-alpha) * parent_b[i]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def crossover_uniform(parent_a: Individual, parent_b: Individual) -> Tuple[Individual, Individual]:\n",
        "    \"\"\"\n",
        "    Uniform crossover: each gene randomly from either parent.\n",
        "    Returns two children.\n",
        "    \"\"\"\n",
        "    mask = torch.rand(parent_a.genome.shape) < 0.5\n",
        "    \n",
        "    child1_genome = torch.where(mask, parent_a.genome, parent_b.genome)\n",
        "    child2_genome = torch.where(mask, parent_b.genome, parent_a.genome)\n",
        "    \n",
        "    return Individual(genome=child1_genome), Individual(genome=child2_genome)\n",
        "\n",
        "\n",
        "def crossover_blend(parent_a: Individual, parent_b: Individual, \n",
        "                    alpha: float = 0.5) -> Individual:\n",
        "    \"\"\"\n",
        "    Blend crossover: interpolate between parents.\n",
        "    alpha=0.5 gives the midpoint.\n",
        "    \"\"\"\n",
        "    child_genome = alpha * parent_a.genome + (1 - alpha) * parent_b.genome\n",
        "    return Individual(genome=child_genome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate crossover\n",
        "parent_a = Individual(genome=torch.tensor([1.0, 1.0, 1.0, 1.0]))\n",
        "parent_b = Individual(genome=torch.tensor([9.0, 9.0, 9.0, 9.0]))\n",
        "\n",
        "print(f\"Parent A: {parent_a.genome.tolist()}\")\n",
        "print(f\"Parent B: {parent_b.genome.tolist()}\")\n",
        "print()\n",
        "\n",
        "# Uniform crossover\n",
        "child1, child2 = crossover_uniform(parent_a, parent_b)\n",
        "print(f\"Uniform crossover:\")\n",
        "print(f\"  Child 1: {child1.genome.tolist()}\")\n",
        "print(f\"  Child 2: {child2.genome.tolist()}\")\n",
        "print()\n",
        "\n",
        "# Blend crossover\n",
        "child_blend = crossover_blend(parent_a, parent_b, alpha=0.5)\n",
        "print(f\"Blend crossover (alpha=0.5):\")\n",
        "print(f\"  Child: {child_blend.genome.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Complete GA Implementation\n",
        "\n",
        "Now let's put it all together into a complete genetic algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def evaluate_population(population: List[Individual], \n",
        "                        fitness_fn: Callable[[torch.Tensor], float]) -> None:\n",
        "    \"\"\"Evaluate fitness for all individuals in place.\"\"\"\n",
        "    for ind in population:\n",
        "        ind.fitness = fitness_fn(ind.genome)\n",
        "\n",
        "\n",
        "def genetic_algorithm(\n",
        "    fitness_fn: Callable[[torch.Tensor], float],\n",
        "    genome_size: int,\n",
        "    pop_size: int = 50,\n",
        "    generations: int = 100,\n",
        "    mutation_rate: float = 0.1,\n",
        "    mutation_magnitude: float = 0.3,\n",
        "    crossover_rate: float = 0.8,\n",
        "    tournament_size: int = 3,\n",
        "    elitism: int = 2,\n",
        "    bounds: Tuple[float, float] = (-5.0, 5.0),\n",
        "    verbose: bool = True\n",
        ") -> Tuple[Individual, List[dict]]:\n",
        "    \"\"\"\n",
        "    Run a genetic algorithm to maximize fitness_fn.\n",
        "    \n",
        "    Args:\n",
        "        fitness_fn: Function mapping genome tensor to scalar fitness\n",
        "        genome_size: Number of genes per individual\n",
        "        pop_size: Population size\n",
        "        generations: Number of generations to run\n",
        "        mutation_rate: Probability each gene mutates\n",
        "        mutation_magnitude: Std dev of mutation noise\n",
        "        crossover_rate: Probability of crossover vs cloning\n",
        "        tournament_size: Size of selection tournaments\n",
        "        elitism: Number of best individuals to preserve unchanged\n",
        "        bounds: (low, high) bounds for initial population\n",
        "        verbose: Print progress\n",
        "    \n",
        "    Returns:\n",
        "        best_individual: The best solution found\n",
        "        history: List of generation statistics\n",
        "    \"\"\"\n",
        "    # Initialize population\n",
        "    population = create_population(pop_size, genome_size, bounds[0], bounds[1])\n",
        "    evaluate_population(population, fitness_fn)\n",
        "    \n",
        "    history = []\n",
        "    \n",
        "    for gen in range(generations):\n",
        "        # Sort by fitness (best first)\n",
        "        population.sort(key=lambda ind: ind.fitness, reverse=True)\n",
        "        \n",
        "        # Record statistics\n",
        "        fitnesses = [ind.fitness for ind in population]\n",
        "        history.append({\n",
        "            'generation': gen,\n",
        "            'best_fitness': fitnesses[0],\n",
        "            'mean_fitness': sum(fitnesses) / len(fitnesses),\n",
        "            'worst_fitness': fitnesses[-1],\n",
        "            'best_genome': population[0].genome.clone()\n",
        "        })\n",
        "        \n",
        "        if verbose and gen % 10 == 0:\n",
        "            print(f\"Gen {gen:3d}: best={fitnesses[0]:.4f}, mean={history[-1]['mean_fitness']:.4f}\")\n",
        "        \n",
        "        # Create next generation\n",
        "        next_population = []\n",
        "        \n",
        "        # Elitism: preserve best individuals\n",
        "        for i in range(elitism):\n",
        "            elite = Individual(genome=population[i].genome.clone())\n",
        "            elite.fitness = population[i].fitness\n",
        "            next_population.append(elite)\n",
        "        \n",
        "        # Fill rest with offspring\n",
        "        while len(next_population) < pop_size:\n",
        "            # Selection\n",
        "            parent_a = tournament_selection(population, tournament_size)\n",
        "            parent_b = tournament_selection(population, tournament_size)\n",
        "            \n",
        "            # Crossover or clone\n",
        "            if random.random() < crossover_rate:\n",
        "                child1, child2 = crossover_uniform(parent_a, parent_b)\n",
        "            else:\n",
        "                child1 = Individual(genome=parent_a.genome.clone())\n",
        "                child2 = Individual(genome=parent_b.genome.clone())\n",
        "            \n",
        "            # Mutation\n",
        "            child1 = mutate(child1, mutation_rate, mutation_magnitude)\n",
        "            child2 = mutate(child2, mutation_rate, mutation_magnitude)\n",
        "            \n",
        "            next_population.append(child1)\n",
        "            if len(next_population) < pop_size:\n",
        "                next_population.append(child2)\n",
        "        \n",
        "        # Evaluate new population\n",
        "        population = next_population\n",
        "        evaluate_population(population, fitness_fn)\n",
        "    \n",
        "    # Final evaluation\n",
        "    population.sort(key=lambda ind: ind.fitness, reverse=True)\n",
        "    return population[0], history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on our 2D multimodal function\n",
        "def fitness_2d(genome: torch.Tensor) -> float:\n",
        "    \"\"\"Fitness function for 2D optimization.\"\"\"\n",
        "    return multimodal_fitness_2d(genome[0], genome[1]).item()\n",
        "\n",
        "set_seed(42)\n",
        "best, history = genetic_algorithm(\n",
        "    fitness_fn=fitness_2d,\n",
        "    genome_size=2,\n",
        "    pop_size=30,\n",
        "    generations=50,\n",
        "    mutation_rate=0.2,\n",
        "    mutation_magnitude=0.5,\n",
        "    bounds=(-1.0, 5.0)\n",
        ")\n",
        "\n",
        "print(f\"\\nBest solution: x={best.genome[0]:.4f}, y={best.genome[1]:.4f}\")\n",
        "print(f\"Best fitness: {best.fitness:.4f}\")\n",
        "print(f\"Expected optimum: x=2.0, y=2.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Visualizing Population Evolution\n",
        "\n",
        "Let's watch how the population explores and converges over generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run GA again but save population snapshots\n",
        "def ga_with_snapshots(\n",
        "    fitness_fn: Callable,\n",
        "    genome_size: int = 2,\n",
        "    pop_size: int = 30,\n",
        "    generations: int = 50,\n",
        "    bounds: Tuple[float, float] = (-1.0, 5.0)\n",
        "):\n",
        "    \"\"\"GA that saves population snapshots for visualization.\"\"\"\n",
        "    population = create_population(pop_size, genome_size, bounds[0], bounds[1])\n",
        "    evaluate_population(population, fitness_fn)\n",
        "    \n",
        "    snapshots = []\n",
        "    \n",
        "    for gen in range(generations):\n",
        "        # Save snapshot\n",
        "        snapshots.append([\n",
        "            (ind.genome[0].item(), ind.genome[1].item(), ind.fitness)\n",
        "            for ind in population\n",
        "        ])\n",
        "        \n",
        "        # Standard GA operations\n",
        "        population.sort(key=lambda ind: ind.fitness, reverse=True)\n",
        "        next_pop = [Individual(genome=population[0].genome.clone())]  # Elitism\n",
        "        \n",
        "        while len(next_pop) < pop_size:\n",
        "            p_a = tournament_selection(population, 3)\n",
        "            p_b = tournament_selection(population, 3)\n",
        "            c1, c2 = crossover_uniform(p_a, p_b)\n",
        "            c1 = mutate(c1, 0.2, 0.5)\n",
        "            c2 = mutate(c2, 0.2, 0.5)\n",
        "            next_pop.extend([c1, c2])\n",
        "        \n",
        "        population = next_pop[:pop_size]\n",
        "        evaluate_population(population, fitness_fn)\n",
        "    \n",
        "    return snapshots\n",
        "\n",
        "set_seed(42)\n",
        "snapshots = ga_with_snapshots(fitness_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize selected generations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
        "gen_indices = [0, 5, 10, 20, 35, 49]\n",
        "\n",
        "for ax, gen_idx in zip(axes.flat, gen_indices):\n",
        "    # Draw fitness landscape\n",
        "    ax.contourf(X.numpy(), Y.numpy(), Z.numpy(), levels=20, cmap='viridis', alpha=0.7)\n",
        "    \n",
        "    # Draw population\n",
        "    snapshot = snapshots[gen_idx]\n",
        "    xs = [s[0] for s in snapshot]\n",
        "    ys = [s[1] for s in snapshot]\n",
        "    fits = [s[2] for s in snapshot]\n",
        "    \n",
        "    # Color by fitness\n",
        "    scatter = ax.scatter(xs, ys, c=fits, cmap='hot', s=50, edgecolor='white', linewidth=0.5)\n",
        "    \n",
        "    ax.set_xlim(-1, 5)\n",
        "    ax.set_ylim(-1, 4)\n",
        "    ax.set_title(f'Generation {gen_idx}')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.plot(2, 2, 'r*', markersize=15)  # Mark optimum\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Population Evolution Over Generations', y=1.02, fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "print(\"Observe: Population starts spread out, then converges to the global optimum.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot fitness over generations\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "gens = [h['generation'] for h in history]\n",
        "best_fits = [h['best_fitness'] for h in history]\n",
        "mean_fits = [h['mean_fitness'] for h in history]\n",
        "\n",
        "ax.plot(gens, best_fits, 'b-', linewidth=2, label='Best fitness')\n",
        "ax.plot(gens, mean_fits, 'g--', linewidth=2, label='Mean fitness')\n",
        "ax.fill_between(gens, mean_fits, best_fits, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel('Generation')\n",
        "ax.set_ylabel('Fitness')\n",
        "ax.set_title('Fitness Improvement Over Generations')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problem 1: Tune GA Parameters\n",
        "\n",
        "Try different GA parameters and see how they affect convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Fill in the parameters to make the GA converge faster\n",
        "# Hint: Consider population size, mutation rate, and tournament size\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "# Fill in these parameters:\n",
        "my_pop_size = 30  # fill in code here\n",
        "my_mutation_rate = 0.1  # fill in code here  \n",
        "my_mutation_magnitude = 0.3  # fill in code here\n",
        "my_tournament_size = 3  # fill in code here\n",
        "\n",
        "best, hist = genetic_algorithm(\n",
        "    fitness_fn=fitness_2d,\n",
        "    genome_size=2,\n",
        "    pop_size=my_pop_size,\n",
        "    generations=30,  # Try to reach optimum in fewer generations\n",
        "    mutation_rate=my_mutation_rate,\n",
        "    mutation_magnitude=my_mutation_magnitude,\n",
        "    tournament_size=my_tournament_size,\n",
        "    bounds=(-1.0, 5.0),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal fitness: {best.fitness:.4f}\")\n",
        "assert best.fitness > 2.5, f\"Fitness should be > 2.5, got {best.fitness:.4f}\"\n",
        "print(\"Success!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. From Numbers to Neural Networks\n",
        "\n",
        "Now for the key insight: **a neural network is just a vector of numbers (weights and biases)**.\n",
        "\n",
        "If we can evolve a vector of 2 numbers to optimize a fitness function,\n",
        "we can evolve a vector of 1000 numbers (neural network weights) the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 A Neural Network as a Flat Vector\n",
        "\n",
        "Consider a simple network:\n",
        "```\n",
        "Input (2) --> Hidden (3) --> Output (1)\n",
        "```\n",
        "\n",
        "Parameters:\n",
        "- W1: 2x3 = 6 weights\n",
        "- b1: 3 biases\n",
        "- W2: 3x1 = 3 weights  \n",
        "- b2: 1 bias\n",
        "- **Total: 13 parameters**\n",
        "\n",
        "We can flatten these into a single vector `[w1_00, w1_01, ..., b2_0]` and evolve it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "class EvolvableNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network that can be evolved (genome = flattened weights).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def get_genome(self) -> torch.Tensor:\n",
        "        \"\"\"Flatten all parameters into a single vector.\"\"\"\n",
        "        params = []\n",
        "        for param in self.parameters():\n",
        "            params.append(param.data.view(-1))\n",
        "        return torch.cat(params)\n",
        "    \n",
        "    def set_genome(self, genome: torch.Tensor) -> None:\n",
        "        \"\"\"Set parameters from a flattened vector.\"\"\"\n",
        "        idx = 0\n",
        "        for param in self.parameters():\n",
        "            size = param.numel()\n",
        "            param.data = genome[idx:idx+size].view(param.shape).clone()\n",
        "            idx += size\n",
        "    \n",
        "    def genome_size(self) -> int:\n",
        "        \"\"\"Total number of parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate genome extraction and setting\n",
        "net = EvolvableNetwork(input_size=2, hidden_size=3, output_size=1)\n",
        "\n",
        "print(f\"Network architecture: 2 -> 3 -> 1\")\n",
        "print(f\"Total parameters: {net.genome_size()}\")\n",
        "print()\n",
        "\n",
        "# Extract genome\n",
        "genome = net.get_genome()\n",
        "print(f\"Genome shape: {genome.shape}\")\n",
        "print(f\"Genome values: {genome[:10].tolist()}...\")  # First 10\n",
        "print()\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.tensor([[1.0, 0.5]])\n",
        "y = net(x)\n",
        "print(f\"Input: {x.tolist()}\")\n",
        "print(f\"Output: {y.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show that modifying genome changes network behavior\n",
        "print(\"Original output:\", net(x).item())\n",
        "\n",
        "# Modify genome\n",
        "new_genome = genome.clone()\n",
        "new_genome[:] = 1.0  # Set all weights to 1.0\n",
        "net.set_genome(new_genome)\n",
        "\n",
        "print(\"After setting all weights to 1.0:\", net(x).item())\n",
        "\n",
        "# Random genome\n",
        "random_genome = torch.randn(net.genome_size())\n",
        "net.set_genome(random_genome)\n",
        "print(\"After random genome:\", net(x).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 The XOR Problem\n",
        "\n",
        "XOR is the classic \"hello world\" of neural networks:\n",
        "- Inputs: (0,0), (0,1), (1,0), (1,1)\n",
        "- Outputs: 0, 1, 1, 0\n",
        "\n",
        "It's not linearly separable, so it requires at least one hidden layer.\n",
        "\n",
        "Let's solve it **without gradients** - using only evolution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XOR dataset\n",
        "XOR_INPUTS = torch.tensor([\n",
        "    [0.0, 0.0],\n",
        "    [0.0, 1.0],\n",
        "    [1.0, 0.0],\n",
        "    [1.0, 1.0]\n",
        "])\n",
        "\n",
        "XOR_TARGETS = torch.tensor([\n",
        "    [0.0],\n",
        "    [1.0],\n",
        "    [1.0],\n",
        "    [0.0]\n",
        "])\n",
        "\n",
        "print(\"XOR Truth Table:\")\n",
        "print(\"  A | B | A XOR B\")\n",
        "print(\"  --|---|--------\")\n",
        "for inp, tgt in zip(XOR_INPUTS, XOR_TARGETS):\n",
        "    print(f\"  {int(inp[0].item())} | {int(inp[1].item())} |    {int(tgt.item())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def xor_fitness(genome: torch.Tensor, net: EvolvableNetwork) -> float:\n",
        "    \"\"\"\n",
        "    Fitness function for XOR problem.\n",
        "    Higher is better (negative MSE).\n",
        "    \"\"\"\n",
        "    net.set_genome(genome)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions = torch.sigmoid(net(XOR_INPUTS))  # Squash to [0, 1]\n",
        "        mse = ((predictions - XOR_TARGETS) ** 2).mean().item()\n",
        "    \n",
        "    # Return negative MSE (we want to maximize fitness)\n",
        "    return -mse\n",
        "\n",
        "\n",
        "def evaluate_xor_network(net: EvolvableNetwork) -> None:\n",
        "    \"\"\"Print XOR predictions for a network.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        predictions = torch.sigmoid(net(XOR_INPUTS))\n",
        "    \n",
        "    print(\"Network predictions:\")\n",
        "    print(\"  A | B | Target | Prediction | Correct?\")\n",
        "    print(\"  --|---|--------|------------|--------\")\n",
        "    \n",
        "    correct = 0\n",
        "    for inp, tgt, pred in zip(XOR_INPUTS, XOR_TARGETS, predictions):\n",
        "        rounded = round(pred.item())\n",
        "        is_correct = rounded == int(tgt.item())\n",
        "        correct += is_correct\n",
        "        print(f\"  {int(inp[0].item())} | {int(inp[1].item())} |   {int(tgt.item())}    |    {pred.item():.3f}    |   {'Yes' if is_correct else 'No'}\")\n",
        "    \n",
        "    print(f\"\\nAccuracy: {correct}/4 = {correct/4*100:.0f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evolve XOR solver\n",
        "set_seed(42)\n",
        "\n",
        "# Create network template\n",
        "xor_net = EvolvableNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "genome_size = xor_net.genome_size()\n",
        "print(f\"Genome size: {genome_size} parameters\")\n",
        "\n",
        "# Fitness function wrapper\n",
        "def xor_fitness_wrapper(genome: torch.Tensor) -> float:\n",
        "    return xor_fitness(genome, xor_net)\n",
        "\n",
        "# Run evolution\n",
        "best_individual, history = genetic_algorithm(\n",
        "    fitness_fn=xor_fitness_wrapper,\n",
        "    genome_size=genome_size,\n",
        "    pop_size=100,\n",
        "    generations=200,\n",
        "    mutation_rate=0.3,\n",
        "    mutation_magnitude=0.5,\n",
        "    tournament_size=5,\n",
        "    bounds=(-2.0, 2.0),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Set best genome and evaluate\n",
        "xor_net.set_genome(best_individual.genome)\n",
        "print(f\"\\nBest fitness: {best_individual.fitness:.6f}\")\n",
        "print()\n",
        "evaluate_xor_network(xor_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot evolution progress\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "gens = [h['generation'] for h in history]\n",
        "best_fits = [h['best_fitness'] for h in history]\n",
        "mean_fits = [h['mean_fitness'] for h in history]\n",
        "\n",
        "ax.plot(gens, best_fits, 'b-', linewidth=2, label='Best fitness (neg MSE)')\n",
        "ax.plot(gens, mean_fits, 'g--', linewidth=1, alpha=0.7, label='Mean fitness')\n",
        "\n",
        "ax.set_xlabel('Generation')\n",
        "ax.set_ylabel('Fitness (negative MSE)')\n",
        "ax.set_title('Evolving XOR Solver Without Gradients')\n",
        "ax.axhline(y=0, color='r', linestyle=':', label='Perfect score')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Evolution vs Gradient Descent on XOR\n",
        "\n",
        "Let's compare our evolved solution with traditional backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train same architecture with gradient descent\n",
        "set_seed(42)\n",
        "\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)\n",
        "        self.fc2 = nn.Linear(4, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Train with gradient descent\n",
        "gd_net = XORNet()\n",
        "optimizer = torch.optim.Adam(gd_net.parameters(), lr=0.1)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "gd_losses = []\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = gd_net(XOR_INPUTS)\n",
        "    loss = criterion(outputs, XOR_TARGETS)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    gd_losses.append(-loss.item())  # Negative for comparison\n",
        "    \n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}: loss={loss.item():.6f}\")\n",
        "\n",
        "print(\"\\nGradient Descent Result:\")\n",
        "with torch.no_grad():\n",
        "    for inp, tgt in zip(XOR_INPUTS, XOR_TARGETS):\n",
        "        pred = gd_net(inp.unsqueeze(0))\n",
        "        print(f\"  ({inp[0]:.0f}, {inp[1]:.0f}) -> {pred.item():.3f} (target: {tgt.item():.0f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare learning curves\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "evolution_best = [h['best_fitness'] for h in history]\n",
        "ax.plot(evolution_best, 'b-', linewidth=2, label='Evolution (best individual)')\n",
        "ax.plot(gd_losses, 'r-', linewidth=2, label='Gradient Descent')\n",
        "\n",
        "ax.set_xlabel('Iteration/Generation')\n",
        "ax.set_ylabel('Fitness (negative MSE)')\n",
        "ax.set_title('Evolution vs Gradient Descent on XOR')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Observations:\")\n",
        "print(\"- Gradient descent converges faster (direct optimization)\")\n",
        "print(\"- Evolution needs more evaluations but requires NO gradients\")\n",
        "print(\"- Both find solutions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problem 2: Evolve an AND Gate\n",
        "\n",
        "Now it's your turn! Evolve a neural network to solve the AND problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AND gate truth table\n",
        "AND_INPUTS = torch.tensor([\n",
        "    [0.0, 0.0],\n",
        "    [0.0, 1.0],\n",
        "    [1.0, 0.0],\n",
        "    [1.0, 1.0]\n",
        "])\n",
        "\n",
        "AND_TARGETS = torch.tensor([\n",
        "    [0.0],\n",
        "    [0.0],\n",
        "    [0.0],\n",
        "    [1.0]\n",
        "])\n",
        "\n",
        "# TODO: Create fitness function for AND gate\n",
        "# Hint: Similar to xor_fitness but with AND_INPUTS and AND_TARGETS\n",
        "\n",
        "and_net = EvolvableNetwork(input_size=2, hidden_size=2, output_size=1)  # Simpler network OK for AND\n",
        "\n",
        "def and_fitness(genome: torch.Tensor) -> float:\n",
        "    # fill in code here\n",
        "    and_net.set_genome(genome)\n",
        "    with torch.no_grad():\n",
        "        predictions = torch.sigmoid(and_net(AND_INPUTS))\n",
        "        mse = ((predictions - AND_TARGETS) ** 2).mean().item()\n",
        "    return -mse\n",
        "\n",
        "# Run evolution\n",
        "set_seed(42)\n",
        "best_and, _ = genetic_algorithm(\n",
        "    fitness_fn=and_fitness,\n",
        "    genome_size=and_net.genome_size(),\n",
        "    pop_size=50,\n",
        "    generations=100,\n",
        "    mutation_rate=0.2,\n",
        "    bounds=(-2.0, 2.0),\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "and_net.set_genome(best_and.genome)\n",
        "with torch.no_grad():\n",
        "    preds = torch.sigmoid(and_net(AND_INPUTS))\n",
        "    correct = ((preds > 0.5).float() == AND_TARGETS).all().item()\n",
        "\n",
        "print(\"AND Gate Predictions:\")\n",
        "for inp, tgt, pred in zip(AND_INPUTS, AND_TARGETS, preds):\n",
        "    print(f\"  ({inp[0]:.0f}, {inp[1]:.0f}) -> {pred.item():.3f} (target: {tgt.item():.0f})\")\n",
        "\n",
        "assert correct, \"Network should solve AND gate\"\n",
        "print(\"\\nSuccess! Network solves AND gate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4. Fixed-Topology Neuroevolution\n",
        "\n",
        "In fixed-topology neuroevolution, the network structure is predefined.\n",
        "We only evolve the **weights and biases**.\n",
        "\n",
        "This is simpler than topology evolution (NEAT) but still powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Weight Mutation Strategies\n",
        "\n",
        "How we mutate weights affects exploration vs exploitation.\n",
        "\n",
        "### Gaussian Mutation (what we've been using)\n",
        "$$w' = w + \\mathcal{N}(0, \\sigma^2)$$\n",
        "\n",
        "### Uniform Mutation\n",
        "$$w' = w + \\text{Uniform}(-\\delta, \\delta)$$\n",
        "\n",
        "### Cauchy Mutation (heavy tails)\n",
        "$$w' = w + \\text{Cauchy}(0, \\gamma)$$\n",
        "- Heavier tails = more large mutations\n",
        "- Good for escaping local optima\n",
        "\n",
        "### Weight Reset\n",
        "$$w' = \\mathcal{N}(0, 1) \\quad \\text{with probability } p_{reset}$$\n",
        "- Occasionally reset a weight completely\n",
        "- Prevents getting stuck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def mutate_gaussian(genome: torch.Tensor, rate: float = 0.1, \n",
        "                    sigma: float = 0.3) -> torch.Tensor:\n",
        "    \"\"\"Standard Gaussian mutation.\"\"\"\n",
        "    mask = torch.rand(genome.shape) < rate\n",
        "    noise = torch.randn(genome.shape) * sigma\n",
        "    return genome + mask.float() * noise\n",
        "\n",
        "\n",
        "def mutate_cauchy(genome: torch.Tensor, rate: float = 0.1,\n",
        "                  gamma: float = 0.1) -> torch.Tensor:\n",
        "    \"\"\"Cauchy mutation for heavier-tailed exploration.\"\"\"\n",
        "    mask = torch.rand(genome.shape) < rate\n",
        "    # Cauchy distribution via inverse CDF\n",
        "    u = torch.rand(genome.shape)\n",
        "    cauchy_noise = gamma * torch.tan(np.pi * (u - 0.5))\n",
        "    return genome + mask.float() * cauchy_noise\n",
        "\n",
        "\n",
        "def mutate_with_reset(genome: torch.Tensor, rate: float = 0.1,\n",
        "                      sigma: float = 0.3, reset_prob: float = 0.05) -> torch.Tensor:\n",
        "    \"\"\"Gaussian mutation with occasional weight reset.\"\"\"\n",
        "    new_genome = mutate_gaussian(genome, rate, sigma)\n",
        "    \n",
        "    # Reset some weights completely\n",
        "    reset_mask = torch.rand(genome.shape) < reset_prob\n",
        "    new_genome = torch.where(reset_mask, torch.randn(genome.shape), new_genome)\n",
        "    \n",
        "    return new_genome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize different mutation distributions\n",
        "set_seed(42)\n",
        "\n",
        "original = torch.zeros(10000)\n",
        "\n",
        "gaussian_mutated = mutate_gaussian(original, rate=1.0, sigma=0.3)\n",
        "cauchy_mutated = mutate_cauchy(original, rate=1.0, gamma=0.1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].hist(gaussian_mutated.numpy(), bins=100, density=True, alpha=0.7, label='Gaussian')\n",
        "axes[0].set_xlim(-2, 2)\n",
        "axes[0].set_title('Gaussian Mutation (sigma=0.3)')\n",
        "axes[0].set_xlabel('Mutation size')\n",
        "axes[0].set_ylabel('Density')\n",
        "\n",
        "axes[1].hist(cauchy_mutated.numpy(), bins=100, density=True, alpha=0.7, color='orange', label='Cauchy')\n",
        "axes[1].set_xlim(-2, 2)\n",
        "axes[1].set_title('Cauchy Mutation (gamma=0.1)')\n",
        "axes[1].set_xlabel('Mutation size')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show that Cauchy has more extreme values\n",
        "print(f\"Gaussian: {(gaussian_mutated.abs() > 1).sum().item()} samples with |mutation| > 1\")\n",
        "print(f\"Cauchy:   {(cauchy_mutated.abs() > 1).sum().item()} samples with |mutation| > 1\")\n",
        "print(\"\\nCauchy's heavy tails = more large jumps = better escape from local optima\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Weight Crossover Strategies\n",
        "\n",
        "When networks have the same topology, crossover is straightforward:\n",
        "just combine weights from both parents.\n",
        "\n",
        "### Uniform Crossover\n",
        "Each weight randomly from parent A or B.\n",
        "\n",
        "### Arithmetic Crossover\n",
        "$$w_{child} = \\alpha \\cdot w_A + (1-\\alpha) \\cdot w_B$$\n",
        "\n",
        "### Layer-wise Crossover\n",
        "Inherit entire layers from each parent (preserves layer structure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def crossover_arithmetic(genome_a: torch.Tensor, genome_b: torch.Tensor,\n",
        "                         alpha: float = None) -> torch.Tensor:\n",
        "    \"\"\"Arithmetic/blend crossover.\"\"\"\n",
        "    if alpha is None:\n",
        "        alpha = random.random()  # Random blend factor\n",
        "    return alpha * genome_a + (1 - alpha) * genome_b\n",
        "\n",
        "\n",
        "def crossover_layer_wise(genome_a: torch.Tensor, genome_b: torch.Tensor,\n",
        "                         layer_sizes: List[int]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Layer-wise crossover: randomly choose each layer from a parent.\n",
        "    layer_sizes: list of parameter counts per layer.\n",
        "    \"\"\"\n",
        "    child = []\n",
        "    idx = 0\n",
        "    \n",
        "    for size in layer_sizes:\n",
        "        # Choose this layer from parent A or B\n",
        "        if random.random() < 0.5:\n",
        "            child.append(genome_a[idx:idx+size])\n",
        "        else:\n",
        "            child.append(genome_b[idx:idx+size])\n",
        "        idx += size\n",
        "    \n",
        "    return torch.cat(child)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate crossover strategies\n",
        "parent_a = torch.zeros(10)  # All zeros\n",
        "parent_b = torch.ones(10)   # All ones\n",
        "\n",
        "print(\"Parent A:\", parent_a.tolist())\n",
        "print(\"Parent B:\", parent_b.tolist())\n",
        "print()\n",
        "\n",
        "# Uniform crossover\n",
        "child_uniform, _ = crossover_uniform(\n",
        "    Individual(genome=parent_a),\n",
        "    Individual(genome=parent_b)\n",
        ")\n",
        "print(\"Uniform crossover:\", child_uniform.genome.tolist())\n",
        "\n",
        "# Arithmetic crossover\n",
        "child_arith = crossover_arithmetic(parent_a, parent_b, alpha=0.3)\n",
        "print(\"Arithmetic (alpha=0.3):\", child_arith.tolist())\n",
        "\n",
        "# Layer-wise (pretend we have layers of size [3, 4, 3])\n",
        "set_seed(42)\n",
        "child_layer = crossover_layer_wise(parent_a, parent_b, [3, 4, 3])\n",
        "print(\"Layer-wise [3,4,3]:\", child_layer.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Demo: Evolve a Cart-Pole Controller\n",
        "\n",
        "Let's evolve a neural network to balance a pole! This is a classic control task.\n",
        "\n",
        "We'll use a simplified simulation (no gym dependency needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "class SimpleCartPole:\n",
        "    \"\"\"\n",
        "    Simplified cart-pole simulation.\n",
        "    State: [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
        "    Action: 0 (push left) or 1 (push right)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.gravity = 9.8\n",
        "        self.cart_mass = 1.0\n",
        "        self.pole_mass = 0.1\n",
        "        self.pole_length = 0.5\n",
        "        self.force_mag = 10.0\n",
        "        self.dt = 0.02\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset to initial state with small random perturbation.\"\"\"\n",
        "        self.x = random.uniform(-0.05, 0.05)  # Cart position\n",
        "        self.x_dot = random.uniform(-0.05, 0.05)  # Cart velocity\n",
        "        self.theta = random.uniform(-0.05, 0.05)  # Pole angle\n",
        "        self.theta_dot = random.uniform(-0.05, 0.05)  # Angular velocity\n",
        "        return self.get_state()\n",
        "    \n",
        "    def get_state(self) -> torch.Tensor:\n",
        "        \"\"\"Return current state as tensor.\"\"\"\n",
        "        return torch.tensor([self.x, self.x_dot, self.theta, self.theta_dot])\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[torch.Tensor, bool]:\n",
        "        \"\"\"Take action (0=left, 1=right), return (new_state, done).\"\"\"\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        \n",
        "        # Physics update (simplified)\n",
        "        cos_theta = np.cos(self.theta)\n",
        "        sin_theta = np.sin(self.theta)\n",
        "        \n",
        "        total_mass = self.cart_mass + self.pole_mass\n",
        "        pole_mass_length = self.pole_mass * self.pole_length\n",
        "        \n",
        "        temp = (force + pole_mass_length * self.theta_dot**2 * sin_theta) / total_mass\n",
        "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / \\\n",
        "                    (self.pole_length * (4/3 - self.pole_mass * cos_theta**2 / total_mass))\n",
        "        x_acc = temp - pole_mass_length * theta_acc * cos_theta / total_mass\n",
        "        \n",
        "        # Euler integration\n",
        "        self.x += self.dt * self.x_dot\n",
        "        self.x_dot += self.dt * x_acc\n",
        "        self.theta += self.dt * self.theta_dot\n",
        "        self.theta_dot += self.dt * theta_acc\n",
        "        \n",
        "        # Check termination\n",
        "        done = abs(self.x) > 2.4 or abs(self.theta) > 0.21  # ~12 degrees\n",
        "        \n",
        "        return self.get_state(), done\n",
        "\n",
        "\n",
        "def evaluate_controller(net: EvolvableNetwork, max_steps: int = 500) -> float:\n",
        "    \"\"\"Run cart-pole episode and return total steps (fitness).\"\"\"\n",
        "    env = SimpleCartPole()\n",
        "    state = env.reset()\n",
        "    \n",
        "    total_steps = 0\n",
        "    for _ in range(max_steps):\n",
        "        # Get action from network\n",
        "        with torch.no_grad():\n",
        "            output = net(state.unsqueeze(0))\n",
        "            action = 1 if output.item() > 0 else 0\n",
        "        \n",
        "        state, done = env.step(action)\n",
        "        total_steps += 1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return float(total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evolve a cart-pole controller\n",
        "set_seed(42)\n",
        "\n",
        "# Create network: 4 inputs -> 8 hidden -> 1 output\n",
        "controller = EvolvableNetwork(input_size=4, hidden_size=8, output_size=1)\n",
        "genome_size = controller.genome_size()\n",
        "print(f\"Controller genome size: {genome_size} parameters\")\n",
        "\n",
        "# Fitness wrapper (average over multiple trials for stability)\n",
        "def cartpole_fitness(genome: torch.Tensor) -> float:\n",
        "    controller.set_genome(genome)\n",
        "    # Average over 3 trials to reduce noise\n",
        "    scores = [evaluate_controller(controller, max_steps=300) for _ in range(3)]\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# Run evolution\n",
        "best_controller, history = genetic_algorithm(\n",
        "    fitness_fn=cartpole_fitness,\n",
        "    genome_size=genome_size,\n",
        "    pop_size=50,\n",
        "    generations=30,\n",
        "    mutation_rate=0.2,\n",
        "    mutation_magnitude=0.5,\n",
        "    tournament_size=3,\n",
        "    bounds=(-2.0, 2.0),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nBest fitness (avg steps balanced): {best_controller.fitness:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the evolved controller\n",
        "controller.set_genome(best_controller.genome)\n",
        "\n",
        "print(\"Testing evolved controller (10 episodes):\")\n",
        "test_scores = []\n",
        "for i in range(10):\n",
        "    score = evaluate_controller(controller, max_steps=500)\n",
        "    test_scores.append(score)\n",
        "    print(f\"  Episode {i+1}: {int(score)} steps\")\n",
        "\n",
        "print(f\"\\nMean: {np.mean(test_scores):.1f} steps\")\n",
        "print(f\"Max:  {max(test_scores):.0f} steps\")\n",
        "print(f\"\\nA random policy typically achieves ~20-30 steps.\")\n",
        "print(f\"500 steps = solved (we cap at 500 for speed).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize learning curve\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "gens = [h['generation'] for h in history]\n",
        "best_fits = [h['best_fitness'] for h in history]\n",
        "mean_fits = [h['mean_fitness'] for h in history]\n",
        "\n",
        "ax.plot(gens, best_fits, 'b-', linewidth=2, label='Best (steps balanced)')\n",
        "ax.plot(gens, mean_fits, 'g--', linewidth=1, alpha=0.7, label='Population mean')\n",
        "\n",
        "ax.set_xlabel('Generation')\n",
        "ax.set_ylabel('Fitness (steps before falling)')\n",
        "ax.set_title('Evolving Cart-Pole Controller')\n",
        "ax.axhline(y=300, color='r', linestyle=':', label='Target (300 steps)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problem 3: Improve the Controller\n",
        "\n",
        "Try different hyperparameters to evolve a better controller faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Find better hyperparameters\n",
        "# Target: Achieve 250+ average fitness in 20 generations or less\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "# Experiment with these:\n",
        "my_pop_size = 50  # fill in code here\n",
        "my_mutation_rate = 0.2  # fill in code here\n",
        "my_mutation_magnitude = 0.5  # fill in code here\n",
        "my_hidden_size = 8  # fill in code here\n",
        "\n",
        "# Create controller\n",
        "test_controller = EvolvableNetwork(4, my_hidden_size, 1)\n",
        "\n",
        "def test_fitness(genome: torch.Tensor) -> float:\n",
        "    test_controller.set_genome(genome)\n",
        "    return sum(evaluate_controller(test_controller, 300) for _ in range(2)) / 2\n",
        "\n",
        "# Evolve\n",
        "best, hist = genetic_algorithm(\n",
        "    fitness_fn=test_fitness,\n",
        "    genome_size=test_controller.genome_size(),\n",
        "    pop_size=my_pop_size,\n",
        "    generations=20,\n",
        "    mutation_rate=my_mutation_rate,\n",
        "    mutation_magnitude=my_mutation_magnitude,\n",
        "    bounds=(-2.0, 2.0),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal best fitness: {best.fitness:.1f}\")\n",
        "# assert best.fitness >= 200, f\"Try to reach 200+ fitness! Got {best.fitness:.1f}\"\n",
        "if best.fitness >= 200:\n",
        "    print(\"Great job! You found good hyperparameters.\")\n",
        "else:\n",
        "    print(\"Keep experimenting with the parameters!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5. NEAT: Topology Evolution\n",
        "\n",
        "So far we've evolved weights in a **fixed** network structure.\n",
        "But what if we could also evolve the **structure** itself?\n",
        "\n",
        "**NEAT** (NeuroEvolution of Augmenting Topologies) does exactly this.\n",
        "\n",
        "Published by Kenneth Stanley in 2002, NEAT introduced three key innovations:\n",
        "1. **Historical markings** (innovation numbers)\n",
        "2. **Speciation** (protecting innovation)\n",
        "3. **Minimal networks** (start simple, grow complex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 The Competing Conventions Problem\n",
        "\n",
        "Why is topology crossover hard? Consider two networks that solve XOR:\n",
        "\n",
        "```\n",
        "Network A:                    Network B:\n",
        "                              \n",
        "   Input1  H1  Output      Input1  H1  Output\n",
        "                                             \n",
        "   Input2  H2               Input2  H2 \n",
        "                                             \n",
        "            H3                       \n",
        "\n",
        "A has 3 hidden nodes           B has 2 hidden nodes\n",
        "```\n",
        "\n",
        "**Problem:** How do we align these for crossover?\n",
        "- Is A's H1 equivalent to B's H1? Or B's H2?\n",
        "- What do we do with A's H3?\n",
        "\n",
        "This is the **competing conventions problem**: different structures can represent\n",
        "the same function, and there's no obvious way to match them up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the problem\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "def draw_network(ax, nodes, edges, title):\n",
        "    \"\"\"Draw a simple network diagram.\"\"\"\n",
        "    ax.set_xlim(-0.5, 3.5)\n",
        "    ax.set_ylim(-0.5, 2.5)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    \n",
        "    # Draw edges\n",
        "    for (x1, y1), (x2, y2) in edges:\n",
        "        ax.plot([x1, x2], [y1, y2], 'b-', linewidth=2, alpha=0.5)\n",
        "    \n",
        "    # Draw nodes\n",
        "    for (x, y), label in nodes:\n",
        "        circle = plt.Circle((x, y), 0.15, color='lightblue', ec='blue', linewidth=2)\n",
        "        ax.add_patch(circle)\n",
        "        ax.text(x, y, label, ha='center', va='center', fontsize=9)\n",
        "\n",
        "# Network A: 3 hidden nodes\n",
        "nodes_a = [\n",
        "    ((0, 2), 'I1'), ((0, 1), 'I2'),  # Inputs\n",
        "    ((1.5, 2.2), 'H1'), ((1.5, 1), 'H2'), ((1.5, -0.2), 'H3'),  # Hidden\n",
        "    ((3, 1), 'O')  # Output\n",
        "]\n",
        "edges_a = [\n",
        "    ((0, 2), (1.5, 2.2)), ((0, 2), (1.5, 1)), ((0, 2), (1.5, -0.2)),\n",
        "    ((0, 1), (1.5, 2.2)), ((0, 1), (1.5, 1)), ((0, 1), (1.5, -0.2)),\n",
        "    ((1.5, 2.2), (3, 1)), ((1.5, 1), (3, 1)), ((1.5, -0.2), (3, 1))\n",
        "]\n",
        "draw_network(axes[0], nodes_a, edges_a, 'Network A (3 hidden nodes)')\n",
        "\n",
        "# Network B: 2 hidden nodes\n",
        "nodes_b = [\n",
        "    ((0, 2), 'I1'), ((0, 1), 'I2'),  # Inputs\n",
        "    ((1.5, 1.8), 'H1'), ((1.5, 0.5), 'H2'),  # Hidden\n",
        "    ((3, 1), 'O')  # Output\n",
        "]\n",
        "edges_b = [\n",
        "    ((0, 2), (1.5, 1.8)), ((0, 2), (1.5, 0.5)),\n",
        "    ((0, 1), (1.5, 1.8)), ((0, 1), (1.5, 0.5)),\n",
        "    ((1.5, 1.8), (3, 1)), ((1.5, 0.5), (3, 1))\n",
        "]\n",
        "draw_network(axes[1], nodes_b, edges_b, 'Network B (2 hidden nodes)')\n",
        "\n",
        "plt.suptitle('Competing Conventions: How to align for crossover?', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Question: Is H1 in Network A the 'same' as H1 in Network B?\")\n",
        "print(\"Without historical information, we can't know!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Historical Markings (Innovation Numbers)\n",
        "\n",
        "NEAT's solution: give each gene a **permanent ID** when it's created.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. Maintain a global **innovation counter**\n",
        "2. When a new connection is created, assign it the next innovation number\n",
        "3. This number **never changes** for that connection type\n",
        "\n",
        "```\n",
        "Generation 0: All networks start with:\n",
        "  Connection (Input1 -> Output): innovation #1\n",
        "  Connection (Input2 -> Output): innovation #2\n",
        "\n",
        "Generation 5: Network A adds a hidden node:\n",
        "  Connection (Input1 -> H1): innovation #3\n",
        "  Connection (H1 -> Output): innovation #4\n",
        "\n",
        "Generation 7: Network B adds same type of hidden node:\n",
        "  Connection (Input1 -> H1): innovation #3  <-- SAME number!\n",
        "  Connection (H1 -> Output): innovation #4  <-- SAME number!\n",
        "```\n",
        "\n",
        "Now crossover can align genes by innovation number!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "@dataclass\n",
        "class ConnectionGene:\n",
        "    \"\"\"A connection in a NEAT genome.\"\"\"\n",
        "    in_node: int      # ID of input node\n",
        "    out_node: int     # ID of output node\n",
        "    weight: float     # Connection weight\n",
        "    enabled: bool     # Is this connection active?\n",
        "    innovation: int   # Historical marking\n",
        "\n",
        "\n",
        "@dataclass  \n",
        "class NodeGene:\n",
        "    \"\"\"A node in a NEAT genome.\"\"\"\n",
        "    id: int           # Unique node ID\n",
        "    node_type: str    # 'input', 'hidden', or 'output'\n",
        "\n",
        "\n",
        "class NEATGenome:\n",
        "    \"\"\"A NEAT genome with nodes and connections.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nodes: List[NodeGene] = []\n",
        "        self.connections: List[ConnectionGene] = []\n",
        "        self.fitness: float = 0.0\n",
        "    \n",
        "    def add_node(self, node: NodeGene):\n",
        "        self.nodes.append(node)\n",
        "    \n",
        "    def add_connection(self, conn: ConnectionGene):\n",
        "        self.connections.append(conn)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"NEATGenome(nodes={len(self.nodes)}, connections={len(self.connections)}, fitness={self.fitness:.2f})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create two genomes and show alignment\n",
        "\n",
        "# Genome A: has connections 1, 2, 3, 4, 6, 7\n",
        "genome_a = NEATGenome()\n",
        "genome_a.add_connection(ConnectionGene(0, 3, 0.5, True, innovation=1))\n",
        "genome_a.add_connection(ConnectionGene(1, 3, 0.3, True, innovation=2))\n",
        "genome_a.add_connection(ConnectionGene(2, 3, 0.7, True, innovation=3))\n",
        "genome_a.add_connection(ConnectionGene(1, 4, 0.2, True, innovation=4))\n",
        "genome_a.add_connection(ConnectionGene(4, 3, -0.4, True, innovation=6))\n",
        "genome_a.add_connection(ConnectionGene(0, 4, 0.1, True, innovation=7))\n",
        "\n",
        "# Genome B: has connections 1, 2, 3, 4, 5, 8\n",
        "genome_b = NEATGenome()\n",
        "genome_b.add_connection(ConnectionGene(0, 3, 0.8, True, innovation=1))\n",
        "genome_b.add_connection(ConnectionGene(1, 3, -0.2, True, innovation=2))\n",
        "genome_b.add_connection(ConnectionGene(2, 3, 0.4, True, innovation=3))\n",
        "genome_b.add_connection(ConnectionGene(1, 4, 0.6, True, innovation=4))\n",
        "genome_b.add_connection(ConnectionGene(4, 5, 0.3, True, innovation=5))\n",
        "genome_b.add_connection(ConnectionGene(5, 3, 0.9, True, innovation=8))\n",
        "\n",
        "print(\"Genome A connections (by innovation #):\")\n",
        "for c in genome_a.connections:\n",
        "    print(f\"  #{c.innovation}: ({c.in_node} -> {c.out_node}), w={c.weight:.1f}\")\n",
        "\n",
        "print(\"\\nGenome B connections (by innovation #):\")\n",
        "for c in genome_b.connections:\n",
        "    print(f\"  #{c.innovation}: ({c.in_node} -> {c.out_node}), w={c.weight:.1f}\")\n",
        "\n",
        "# Align by innovation number\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ALIGNMENT BY INNOVATION NUMBER:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "a_innovations = {c.innovation: c for c in genome_a.connections}\n",
        "b_innovations = {c.innovation: c for c in genome_b.connections}\n",
        "all_innovations = sorted(set(a_innovations.keys()) | set(b_innovations.keys()))\n",
        "\n",
        "print(f\"{'Innov':^6} | {'Genome A':^20} | {'Genome B':^20} | {'Type':^10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for innov in all_innovations:\n",
        "    a_gene = a_innovations.get(innov)\n",
        "    b_gene = b_innovations.get(innov)\n",
        "    \n",
        "    a_str = f\"w={a_gene.weight:.1f}\" if a_gene else \"-\"\n",
        "    b_str = f\"w={b_gene.weight:.1f}\" if b_gene else \"-\"\n",
        "    \n",
        "    if a_gene and b_gene:\n",
        "        gene_type = \"MATCHING\"\n",
        "    elif a_gene:\n",
        "        gene_type = \"DISJOINT\" if innov < max(b_innovations.keys()) else \"EXCESS\"\n",
        "    else:\n",
        "        gene_type = \"DISJOINT\" if innov < max(a_innovations.keys()) else \"EXCESS\"\n",
        "    \n",
        "    print(f\"{innov:^6} | {a_str:^20} | {b_str:^20} | {gene_type:^10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 NEAT Crossover\n",
        "\n",
        "With innovation numbers, crossover is straightforward:\n",
        "\n",
        "1. **Matching genes**: randomly inherit from either parent\n",
        "2. **Disjoint genes**: inherit from the more fit parent\n",
        "3. **Excess genes**: inherit from the more fit parent\n",
        "\n",
        "```\n",
        "Parent A (more fit):    1  2  3  4  -  6  7\n",
        "Parent B (less fit):    1  2  3  4  5  -  -  8\n",
        "                        \n",
        "Child:                  1* 2* 3* 4* -  6  7     (* = randomly from A or B)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def neat_crossover(parent_a: NEATGenome, parent_b: NEATGenome) -> NEATGenome:\n",
        "    \"\"\"\n",
        "    NEAT crossover. Assumes parent_a is more fit (or equal).\n",
        "    \"\"\"\n",
        "    child = NEATGenome()\n",
        "    \n",
        "    # Index connections by innovation number\n",
        "    a_genes = {c.innovation: c for c in parent_a.connections}\n",
        "    b_genes = {c.innovation: c for c in parent_b.connections}\n",
        "    \n",
        "    # Process all innovations from more fit parent\n",
        "    for innovation in a_genes:\n",
        "        a_gene = a_genes[innovation]\n",
        "        b_gene = b_genes.get(innovation)\n",
        "        \n",
        "        if b_gene is not None:\n",
        "            # Matching gene: randomly choose\n",
        "            chosen = random.choice([a_gene, b_gene])\n",
        "        else:\n",
        "            # Disjoint or excess: take from more fit parent\n",
        "            chosen = a_gene\n",
        "        \n",
        "        # Clone the gene\n",
        "        new_gene = ConnectionGene(\n",
        "            in_node=chosen.in_node,\n",
        "            out_node=chosen.out_node,\n",
        "            weight=chosen.weight,\n",
        "            enabled=chosen.enabled,\n",
        "            innovation=chosen.innovation\n",
        "        )\n",
        "        child.add_connection(new_gene)\n",
        "    \n",
        "    return child"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate NEAT crossover\n",
        "set_seed(42)\n",
        "\n",
        "# Make genome_a more fit\n",
        "genome_a.fitness = 10.0\n",
        "genome_b.fitness = 5.0\n",
        "\n",
        "child = neat_crossover(genome_a, genome_b)  # A is more fit\n",
        "\n",
        "print(\"Parent A (fitness=10):\")\n",
        "for c in genome_a.connections:\n",
        "    print(f\"  #{c.innovation}: w={c.weight:.1f}\")\n",
        "\n",
        "print(\"\\nParent B (fitness=5):\")\n",
        "for c in genome_b.connections:\n",
        "    print(f\"  #{c.innovation}: w={c.weight:.1f}\")\n",
        "\n",
        "print(\"\\nChild (inherits structure from A, weights mixed):\")\n",
        "for c in child.connections:\n",
        "    print(f\"  #{c.innovation}: w={c.weight:.1f}\")\n",
        "\n",
        "print(f\"\\nChild has {len(child.connections)} connections (same as parent A)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Speciation: Protecting Innovation\n",
        "\n",
        "**Problem:** New structures are initially bad (random new weights).\n",
        "They get outcompeted and die before they can optimize.\n",
        "\n",
        "**Solution:** Group similar networks into **species**.\n",
        "Competition happens **within** species, not across the whole population.\n",
        "\n",
        "### Compatibility Distance\n",
        "\n",
        "NEAT measures how different two genomes are:\n",
        "\n",
        "$$\\delta = \\frac{c_1 \\cdot E}{N} + \\frac{c_2 \\cdot D}{N} + c_3 \\cdot \\bar{W}$$\n",
        "\n",
        "Where:\n",
        "- $E$ = number of excess genes\n",
        "- $D$ = number of disjoint genes\n",
        "- $\\bar{W}$ = average weight difference of matching genes\n",
        "- $N$ = number of genes in larger genome\n",
        "- $c_1, c_2, c_3$ = importance coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "def compatibility_distance(\n",
        "    genome_a: NEATGenome, \n",
        "    genome_b: NEATGenome,\n",
        "    c1: float = 1.0,  # Excess gene coefficient\n",
        "    c2: float = 1.0,  # Disjoint gene coefficient  \n",
        "    c3: float = 0.4   # Weight difference coefficient\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculate compatibility distance between two NEAT genomes.\n",
        "    Lower = more similar.\n",
        "    \"\"\"\n",
        "    a_genes = {c.innovation: c for c in genome_a.connections}\n",
        "    b_genes = {c.innovation: c for c in genome_b.connections}\n",
        "    \n",
        "    max_a = max(a_genes.keys()) if a_genes else 0\n",
        "    max_b = max(b_genes.keys()) if b_genes else 0\n",
        "    \n",
        "    excess = 0\n",
        "    disjoint = 0\n",
        "    matching = 0\n",
        "    weight_diff = 0.0\n",
        "    \n",
        "    all_innovations = set(a_genes.keys()) | set(b_genes.keys())\n",
        "    \n",
        "    for innov in all_innovations:\n",
        "        in_a = innov in a_genes\n",
        "        in_b = innov in b_genes\n",
        "        \n",
        "        if in_a and in_b:\n",
        "            # Matching\n",
        "            matching += 1\n",
        "            weight_diff += abs(a_genes[innov].weight - b_genes[innov].weight)\n",
        "        elif in_a:\n",
        "            # Only in A\n",
        "            if innov > max_b:\n",
        "                excess += 1\n",
        "            else:\n",
        "                disjoint += 1\n",
        "        else:\n",
        "            # Only in B\n",
        "            if innov > max_a:\n",
        "                excess += 1\n",
        "            else:\n",
        "                disjoint += 1\n",
        "    \n",
        "    N = max(len(a_genes), len(b_genes), 1)\n",
        "    avg_weight_diff = weight_diff / matching if matching > 0 else 0\n",
        "    \n",
        "    return (c1 * excess / N) + (c2 * disjoint / N) + (c3 * avg_weight_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate distances between our example genomes\n",
        "dist_ab = compatibility_distance(genome_a, genome_b)\n",
        "dist_aa = compatibility_distance(genome_a, genome_a)\n",
        "\n",
        "print(f\"Distance(A, A) = {dist_aa:.3f}  (same genome = 0)\")\n",
        "print(f\"Distance(A, B) = {dist_ab:.3f}  (different genomes)\")\n",
        "\n",
        "# Show breakdown\n",
        "a_genes = {c.innovation for c in genome_a.connections}\n",
        "b_genes = {c.innovation for c in genome_b.connections}\n",
        "\n",
        "matching = a_genes & b_genes\n",
        "only_a = a_genes - b_genes\n",
        "only_b = b_genes - a_genes\n",
        "\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  Matching genes: {matching}\")\n",
        "print(f\"  Only in A: {only_a}\")\n",
        "print(f\"  Only in B: {only_b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 The Minimal Networks Principle\n",
        "\n",
        "NEAT starts with **minimal** networks:\n",
        "- Only input nodes and output nodes\n",
        "- Direct connections from inputs to outputs\n",
        "- **No hidden nodes**\n",
        "\n",
        "Complexity is added through mutations:\n",
        "1. **Add Connection**: Create new connection between existing nodes\n",
        "2. **Add Node**: Split an existing connection with a new node\n",
        "\n",
        "```\n",
        "Before add_node:        After add_node:\n",
        "                        \n",
        "  A > B             A > N > B\n",
        "      (w)                  (1)   (w)\n",
        "```\n",
        "\n",
        "**Why start minimal?**\n",
        "- Smaller search space initially\n",
        "- Complexity only added when beneficial\n",
        "- Avoids bloat (unnecessary structure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "class InnovationTracker:\n",
        "    \"\"\"Tracks innovation numbers for new connections.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.counter = 0\n",
        "        self.innovations = {}  # (in_node, out_node) -> innovation number\n",
        "    \n",
        "    def get_innovation(self, in_node: int, out_node: int) -> int:\n",
        "        \"\"\"Get or create innovation number for a connection.\"\"\"\n",
        "        key = (in_node, out_node)\n",
        "        if key not in self.innovations:\n",
        "            self.counter += 1\n",
        "            self.innovations[key] = self.counter\n",
        "        return self.innovations[key]\n",
        "\n",
        "\n",
        "def mutate_add_connection(genome: NEATGenome, tracker: InnovationTracker) -> None:\n",
        "    \"\"\"Add a new connection between two unconnected nodes.\"\"\"\n",
        "    # Get all possible node pairs\n",
        "    node_ids = [n.id for n in genome.nodes]\n",
        "    existing = {(c.in_node, c.out_node) for c in genome.connections}\n",
        "    \n",
        "    # Find unconnected pairs (no self-connections)\n",
        "    candidates = [(a, b) for a in node_ids for b in node_ids \n",
        "                  if a != b and (a, b) not in existing]\n",
        "    \n",
        "    if not candidates:\n",
        "        return  # Fully connected\n",
        "    \n",
        "    in_node, out_node = random.choice(candidates)\n",
        "    innovation = tracker.get_innovation(in_node, out_node)\n",
        "    \n",
        "    new_conn = ConnectionGene(\n",
        "        in_node=in_node,\n",
        "        out_node=out_node,\n",
        "        weight=random.gauss(0, 1),\n",
        "        enabled=True,\n",
        "        innovation=innovation\n",
        "    )\n",
        "    genome.add_connection(new_conn)\n",
        "\n",
        "\n",
        "def mutate_add_node(genome: NEATGenome, tracker: InnovationTracker, \n",
        "                    next_node_id: int) -> int:\n",
        "    \"\"\"Split an existing connection with a new node.\"\"\"\n",
        "    if not genome.connections:\n",
        "        return next_node_id\n",
        "    \n",
        "    # Choose a connection to split\n",
        "    enabled_conns = [c for c in genome.connections if c.enabled]\n",
        "    if not enabled_conns:\n",
        "        return next_node_id\n",
        "    \n",
        "    conn = random.choice(enabled_conns)\n",
        "    conn.enabled = False  # Disable old connection\n",
        "    \n",
        "    # Add new node\n",
        "    new_node = NodeGene(id=next_node_id, node_type='hidden')\n",
        "    genome.add_node(new_node)\n",
        "    \n",
        "    # Add two new connections\n",
        "    # Connection into new node (weight = 1.0)\n",
        "    innov1 = tracker.get_innovation(conn.in_node, next_node_id)\n",
        "    genome.add_connection(ConnectionGene(\n",
        "        in_node=conn.in_node,\n",
        "        out_node=next_node_id,\n",
        "        weight=1.0,\n",
        "        enabled=True,\n",
        "        innovation=innov1\n",
        "    ))\n",
        "    \n",
        "    # Connection from new node (weight = original weight)\n",
        "    innov2 = tracker.get_innovation(next_node_id, conn.out_node)\n",
        "    genome.add_connection(ConnectionGene(\n",
        "        in_node=next_node_id,\n",
        "        out_node=conn.out_node,\n",
        "        weight=conn.weight,\n",
        "        enabled=True,\n",
        "        innovation=innov2\n",
        "    ))\n",
        "    \n",
        "    return next_node_id + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate minimal network evolution\n",
        "set_seed(42)\n",
        "\n",
        "# Create minimal XOR network\n",
        "def create_minimal_xor_genome(tracker: InnovationTracker) -> NEATGenome:\n",
        "    \"\"\"Create minimal genome for XOR: 2 inputs, 1 output, direct connections.\"\"\"\n",
        "    genome = NEATGenome()\n",
        "    \n",
        "    # Nodes: 0, 1 = inputs; 2 = output\n",
        "    genome.add_node(NodeGene(id=0, node_type='input'))\n",
        "    genome.add_node(NodeGene(id=1, node_type='input'))\n",
        "    genome.add_node(NodeGene(id=2, node_type='output'))\n",
        "    \n",
        "    # Direct connections\n",
        "    genome.add_connection(ConnectionGene(\n",
        "        in_node=0, out_node=2, weight=random.gauss(0, 1),\n",
        "        enabled=True, innovation=tracker.get_innovation(0, 2)\n",
        "    ))\n",
        "    genome.add_connection(ConnectionGene(\n",
        "        in_node=1, out_node=2, weight=random.gauss(0, 1),\n",
        "        enabled=True, innovation=tracker.get_innovation(1, 2)\n",
        "    ))\n",
        "    \n",
        "    return genome\n",
        "\n",
        "tracker = InnovationTracker()\n",
        "genome = create_minimal_xor_genome(tracker)\n",
        "\n",
        "print(\"Initial minimal genome:\")\n",
        "print(f\"  Nodes: {[(n.id, n.node_type) for n in genome.nodes]}\")\n",
        "print(f\"  Connections: {[(c.in_node, c.out_node, c.innovation) for c in genome.connections]}\")\n",
        "\n",
        "# Add a node\n",
        "next_id = 3\n",
        "next_id = mutate_add_node(genome, tracker, next_id)\n",
        "\n",
        "print(\"\\nAfter add_node mutation:\")\n",
        "print(f\"  Nodes: {[(n.id, n.node_type) for n in genome.nodes]}\")\n",
        "print(f\"  Connections (enabled only): {[(c.in_node, c.out_node, c.innovation) for c in genome.connections if c.enabled]}\")\n",
        "\n",
        "# Add another connection\n",
        "mutate_add_connection(genome, tracker)\n",
        "\n",
        "print(\"\\nAfter add_connection mutation:\")\n",
        "print(f\"  Connections (enabled): {[(c.in_node, c.out_node, c.innovation) for c in genome.connections if c.enabled]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.6 NEAT Algorithm Pseudocode\n",
        "\n",
        "Here's the complete NEAT algorithm:\n",
        "\n",
        "```python\n",
        "def NEAT(fitness_function, generations):\n",
        "    # 1. Initialize minimal population\n",
        "    population = [create_minimal_genome() for _ in range(pop_size)]\n",
        "    species_list = []  # Groups of similar genomes\n",
        "    \n",
        "    for generation in range(generations):\n",
        "        # 2. Evaluate fitness\n",
        "        for genome in population:\n",
        "            genome.fitness = fitness_function(genome)\n",
        "        \n",
        "        # 3. Speciation: group genomes by similarity\n",
        "        species_list = speciate(population, species_list)\n",
        "        \n",
        "        # 4. Adjust fitness (share within species)\n",
        "        for species in species_list:\n",
        "            for genome in species:\n",
        "                genome.adjusted_fitness = genome.fitness / len(species)\n",
        "        \n",
        "        # 5. Reproduce\n",
        "        new_population = []\n",
        "        for species in species_list:\n",
        "            # Number of offspring proportional to species' total adjusted fitness\n",
        "            offspring_count = calculate_offspring(species)\n",
        "            \n",
        "            for _ in range(offspring_count):\n",
        "                # Select parents from this species\n",
        "                parent_a = tournament_select(species)\n",
        "                \n",
        "                if random() < crossover_rate:\n",
        "                    parent_b = tournament_select(species)\n",
        "                    child = crossover(parent_a, parent_b)\n",
        "                else:\n",
        "                    child = clone(parent_a)\n",
        "                \n",
        "                # Mutate\n",
        "                if random() < weight_mutation_rate:\n",
        "                    mutate_weights(child)\n",
        "                if random() < add_node_rate:\n",
        "                    mutate_add_node(child)\n",
        "                if random() < add_connection_rate:\n",
        "                    mutate_add_connection(child)\n",
        "                \n",
        "                new_population.append(child)\n",
        "        \n",
        "        population = new_population\n",
        "    \n",
        "    return best_genome(population)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Why NEAT Works\n",
        "\n",
        "| Innovation | Why It Matters |\n",
        "|------------|----------------|\n",
        "| Historical markings | Enable meaningful crossover between different structures |\n",
        "| Speciation | Protects new structures from immediate competition |\n",
        "| Minimal init | Starts simple, adds complexity only when needed |\n",
        "\n",
        "**Key insight:** NEAT solves the \"credit assignment\" problem for structure.\n",
        "New structures get time to optimize their weights before being judged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. Connection to Evolution Lab\n",
        "\n",
        "Now let's see how these concepts map to the Evolution Lab codebase.\n",
        "\n",
        "Evolution Lab evolves virtual creatures to collect food pellets.\n",
        "Each creature has a **body** (nodes and muscles) and a **control system** (oscillators or neural network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Evolution Lab Architecture\n",
        "\n",
        "From the codebase documentation (`docs/NEURAL.md`):\n",
        "\n",
        "```\n",
        "                    EVOLUTION LAB\n",
        "                    \n",
        "                    \n",
        "Creature Genome:      Body Structure    +    Control System\n",
        "                               \n",
        "                      - Node positions       - Oscillator params (default)\n",
        "                      - Muscle connections   - Neural weights (optional)\n",
        "                      \n",
        "                              \n",
        "                              \n",
        "                              \n",
        "              \n",
        "                   Physics Simulation          \n",
        "                 (Cannon.js - Black Box)       \n",
        "              \n",
        "                              \n",
        "                              \n",
        "                              \n",
        "                    Fitness = Distance to pellet\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Control Modes: Oscillators vs Neural Networks\n",
        "\n",
        "Evolution Lab supports two control systems:\n",
        "\n",
        "### Oscillator Mode (Default)\n",
        "Each muscle contracts according to:\n",
        "```python\n",
        "contraction = amplitude * sin(time * frequency * 2*pi + phase)\n",
        "```\n",
        "\n",
        "**Evolved parameters per muscle:**\n",
        "- `frequency`: How fast to oscillate\n",
        "- `amplitude`: How strong the contraction\n",
        "- `phase`: When to contract relative to others\n",
        "- Direction/velocity/distance biases\n",
        "\n",
        "**Total: ~12 parameters per muscle**\n",
        "\n",
        "### Neural Network Mode\n",
        "A neural network maps sensor inputs to muscle activations:\n",
        "\n",
        "```\n",
        "INPUTS (8)              HIDDEN (8, tanh)         OUTPUTS (N muscles)\n",
        "                      \n",
        "\n",
        "pellet_dir_x                            muscle_0_mod [-1, 1]\n",
        "pellet_dir_y          \n",
        "pellet_dir_z                  muscle_1_mod [-1, 1]\n",
        "velocity_x           8 neurons       \n",
        "velocity_y         tanh       muscle_2_mod [-1, 1]\n",
        "velocity_z          activation       \n",
        "pellet_dist                   ...\n",
        "time_phase            \n",
        "                                              muscle_N_mod [-1, 1]\n",
        "```\n",
        "\n",
        "**Total: 72 + 9N parameters** (for N muscles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameter count comparison\n",
        "def oscillator_params(n_muscles: int) -> int:\n",
        "    \"\"\"Parameters for oscillator mode.\"\"\"\n",
        "    return 12 * n_muscles  # ~12 params per muscle\n",
        "\n",
        "def neural_params(n_muscles: int, hidden_size: int = 8, n_inputs: int = 8) -> int:\n",
        "    \"\"\"Parameters for neural network mode.\"\"\"\n",
        "    input_to_hidden = n_inputs * hidden_size + hidden_size  # W1 + b1\n",
        "    hidden_to_output = hidden_size * n_muscles + n_muscles  # W2 + b2\n",
        "    return input_to_hidden + hidden_to_output\n",
        "\n",
        "print(\"Parameter Count Comparison\")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Muscles':^10} | {'Oscillator':^12} | {'Neural Net':^12}\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "for n in [5, 10, 15, 20]:\n",
        "    osc = oscillator_params(n)\n",
        "    nn = neural_params(n)\n",
        "    print(f\"{n:^10} | {osc:^12} | {nn:^12}\")\n",
        "\n",
        "print(\"\\nSimilar parameter counts, but very different capabilities!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 Hybrid Mode: The Best of Both Worlds\n",
        "\n",
        "Evolution Lab's **hybrid mode** combines oscillators with neural control:\n",
        "\n",
        "```python\n",
        "# Base oscillation (provides rhythm)\n",
        "base = sin(time * frequency * 2*pi + phase)\n",
        "\n",
        "# Neural modulation (provides steering/adaptation)\n",
        "nn_output = network.forward(sensors)  # in [-1, 1]\n",
        "\n",
        "# Combined\n",
        "modulation = 1.0 + nn_output * modulation_strength  # e.g., [0.5, 1.5]\n",
        "contraction = base * amplitude * modulation\n",
        "```\n",
        "\n",
        "**Why hybrid?**\n",
        "\n",
        "| Problem | Solution |\n",
        "|---------|----------|\n",
        "| Random neural weights = no movement | Oscillator provides baseline locomotion |\n",
        "| Neural network hard to bootstrap | Smoother fitness landscape |\n",
        "| Pure neural can't do rhythm easily | Oscillator handles timing |\n",
        "\n",
        "**Think of it as:** The oscillator provides the \"walk cycle\" while the neural network provides \"steering\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize hybrid control\n",
        "t = torch.linspace(0, 4, 500)  # 4 seconds\n",
        "\n",
        "# Base oscillator\n",
        "frequency = 1.0\n",
        "amplitude = 1.0\n",
        "phase = 0.0\n",
        "base = amplitude * torch.sin(2 * np.pi * frequency * t + phase)\n",
        "\n",
        "# Simulated neural modulation (responds to some \"stimulus\" at t=2)\n",
        "# In reality this would come from the network\n",
        "stimulus = torch.sigmoid((t - 2) * 5)  # Ramps up at t=2\n",
        "nn_output = 0.5 * stimulus - 0.25  # Modulation in [-0.25, 0.25]\n",
        "\n",
        "modulation_strength = 0.5\n",
        "modulation = 1.0 + nn_output * modulation_strength\n",
        "\n",
        "# Combined output\n",
        "hybrid = base * modulation\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
        "\n",
        "axes[0].plot(t, base, 'b-', linewidth=2)\n",
        "axes[0].set_ylabel('Contraction')\n",
        "axes[0].set_title('Base Oscillator (provides rhythm)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(t, modulation, 'g-', linewidth=2)\n",
        "axes[1].axvline(x=2, color='r', linestyle='--', alpha=0.5, label='Stimulus')\n",
        "axes[1].set_ylabel('Modulation')\n",
        "axes[1].set_title('Neural Modulation (responds to stimulus at t=2)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(t, hybrid, 'purple', linewidth=2)\n",
        "axes[2].set_xlabel('Time (s)')\n",
        "axes[2].set_ylabel('Contraction')\n",
        "axes[2].set_title('Hybrid Output (oscillator + neural steering)')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: The rhythm is preserved, but the amplitude changes based on neural output.\")\n",
        "print(\"This allows creatures to 'turn' by modulating different muscles differently.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.4 Creature Genome Structure\n",
        "\n",
        "A creature in Evolution Lab has two evolvable components:\n",
        "\n",
        "### Body Genome\n",
        "```typescript\n",
        "interface BodyGenome {\n",
        "  nodes: NodePosition[];     // Body segments (spheres)\n",
        "  muscles: MuscleConnection[]; // Springs connecting nodes\n",
        "}\n",
        "```\n",
        "\n",
        "### Control Genome (depends on mode)\n",
        "\n",
        "**Oscillator mode:**\n",
        "```typescript\n",
        "interface OscillatorParams {\n",
        "  frequency: number;    // [0.5, 3.0] Hz\n",
        "  amplitude: number;    // [0.1, 1.0]\n",
        "  phase: number;        // [0, 2*pi]\n",
        "  directionBias: Vec3;  // Response to pellet direction\n",
        "  velocityBias: Vec3;   // Response to creature velocity\n",
        "  distanceBias: number; // Response to pellet distance\n",
        "}\n",
        "```\n",
        "\n",
        "**Neural mode:**\n",
        "```typescript\n",
        "interface NeuralWeights {\n",
        "  inputHiddenWeights: number[][];  // 8 x hiddenSize\n",
        "  inputHiddenBias: number[];       // hiddenSize\n",
        "  hiddenOutputWeights: number[][]; // hiddenSize x numMuscles  \n",
        "  hiddenOutputBias: number[];      // numMuscles\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate creature genome structure\n",
        "@dataclass\n",
        "class NodePosition:\n",
        "    x: float\n",
        "    y: float  \n",
        "    z: float\n",
        "    radius: float = 0.3\n",
        "\n",
        "@dataclass\n",
        "class MuscleConnection:\n",
        "    node_a: int\n",
        "    node_b: int\n",
        "    stiffness: float = 100.0\n",
        "    damping: float = 5.0\n",
        "\n",
        "@dataclass\n",
        "class OscillatorParams:\n",
        "    frequency: float = 1.0\n",
        "    amplitude: float = 0.5\n",
        "    phase: float = 0.0\n",
        "    direction_bias: Tuple[float, float, float] = (0.0, 0.0, 0.0)\n",
        "\n",
        "@dataclass\n",
        "class CreatureGenome:\n",
        "    nodes: List[NodePosition]\n",
        "    muscles: List[MuscleConnection]\n",
        "    oscillator_params: List[OscillatorParams]  # One per muscle\n",
        "    neural_weights: torch.Tensor = None  # Optional\n",
        "    \n",
        "    def total_params(self) -> int:\n",
        "        \"\"\"Total evolvable parameters.\"\"\"\n",
        "        body_params = len(self.nodes) * 4 + len(self.muscles) * 2\n",
        "        if self.neural_weights is not None:\n",
        "            return body_params + self.neural_weights.numel()\n",
        "        return body_params + len(self.oscillator_params) * 7  # ~7 params per oscillator\n",
        "\n",
        "# Example creature\n",
        "example_creature = CreatureGenome(\n",
        "    nodes=[\n",
        "        NodePosition(0, 0.5, 0),\n",
        "        NodePosition(1, 0.5, 0),\n",
        "        NodePosition(2, 0.5, 0),\n",
        "        NodePosition(0.5, 0.5, 0.5),\n",
        "        NodePosition(1.5, 0.5, 0.5),\n",
        "    ],\n",
        "    muscles=[\n",
        "        MuscleConnection(0, 1),\n",
        "        MuscleConnection(1, 2),\n",
        "        MuscleConnection(0, 3),\n",
        "        MuscleConnection(1, 3),\n",
        "        MuscleConnection(1, 4),\n",
        "        MuscleConnection(2, 4),\n",
        "    ],\n",
        "    oscillator_params=[OscillatorParams() for _ in range(6)]\n",
        ")\n",
        "\n",
        "print(\"Example Creature Genome:\")\n",
        "print(f\"  Nodes: {len(example_creature.nodes)}\")\n",
        "print(f\"  Muscles: {len(example_creature.muscles)}\")\n",
        "print(f\"  Total parameters: ~{example_creature.total_params()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5 Evolution Operators in Evolution Lab\n",
        "\n",
        "Evolution Lab uses similar operators to what we've built:\n",
        "\n",
        "### Selection\n",
        "- **Tournament selection** (k=3 by default)\n",
        "- **Elitism**: Top 2 creatures preserved unchanged\n",
        "\n",
        "### Mutation\n",
        "```python\n",
        "# Weight/param mutation (from docs)\n",
        "def mutate_weights(weights, mutation_rate, mutation_magnitude):\n",
        "    for i in range(len(weights)):\n",
        "        if random() < mutation_rate:\n",
        "            weights[i] += random_normal() * mutation_magnitude\n",
        "    return weights\n",
        "```\n",
        "\n",
        "### Crossover\n",
        "**Uniform crossover** for parameters:\n",
        "```python\n",
        "def crossover_weights(parent1, parent2):\n",
        "    child = []\n",
        "    for w1, w2 in zip(parent1, parent2):\n",
        "        child.append(w1 if random() < 0.5 else w2)\n",
        "    return child\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.6 Future: NEAT in Evolution Lab\n",
        "\n",
        "The Evolution Lab docs mention future NEAT support:\n",
        "\n",
        "| Current (Fixed Topology) | Future (NEAT) |\n",
        "|--------------------------|---------------|\n",
        "| Flat weight array | Connection genes with innovation numbers |\n",
        "| Fixed forward pass | Dynamic topology evaluation |\n",
        "| Weight mutation only | Add structural mutations |\n",
        "| Single population | Speciated population |\n",
        "\n",
        "**Why NEAT for Evolution Lab?**\n",
        "\n",
        "1. **Variable creature sizes**: Different creatures have different numbers of muscles.\n",
        "   NEAT can evolve networks that match each creature's complexity.\n",
        "\n",
        "2. **Minimal networks**: Simple creatures don't need 8 hidden nodes.\n",
        "   NEAT starts minimal and grows only when needed.\n",
        "\n",
        "3. **Innovation protection**: New body structures (from body mutation) need time\n",
        "   to evolve appropriate control. Speciation provides this protection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.7 Key Insights from Evolution Lab\n",
        "\n",
        "### Why Neuroevolution Works Here\n",
        "\n",
        "1. **Non-differentiable physics**: Cannon.js physics engine is a black box.\n",
        "   We can't backpropagate through collision detection.\n",
        "\n",
        "2. **Sparse rewards**: We only know fitness after an 8-second simulation.\n",
        "   No dense reward signal for gradient methods.\n",
        "\n",
        "3. **Complex dynamics**: Creature behavior emerges from many interacting parts.\n",
        "   The fitness landscape is rugged with many local optima.\n",
        "\n",
        "### The Oscillator-Neural Connection\n",
        "\n",
        "```\n",
        "Oscillator params      A specialized neural network\n",
        "       \n",
        "Fixed architecture      (time  sin  amplitude  output)\n",
        "Few parameters          Fast to evolve\n",
        "Limited behaviors       Only periodic motion\n",
        "\n",
        "Neural network      =   A general function approximator  \n",
        "          \n",
        "Flexible architecture   Any input  output mapping\n",
        "More parameters         Slower to evolve\n",
        "Any behavior possible   Can learn complex responses\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final visualization: Oscillator as a \"fixed\" neural network\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Oscillator \"network\"\n",
        "ax1 = axes[0]\n",
        "ax1.set_xlim(-0.5, 4.5)\n",
        "ax1.set_ylim(-0.5, 2.5)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Oscillator Mode\\n(Fixed \"network\")', fontsize=12)\n",
        "\n",
        "# Draw oscillator flow\n",
        "nodes_osc = [\n",
        "    ((0, 1), 'time'),\n",
        "    ((1.5, 1.5), 'sin'),\n",
        "    ((1.5, 0.5), 'bias'),\n",
        "    ((3, 1), 'amp'),\n",
        "    ((4, 1), 'out'),\n",
        "]\n",
        "for (x, y), label in nodes_osc:\n",
        "    circle = plt.Circle((x, y), 0.2, color='lightgreen', ec='green', linewidth=2)\n",
        "    ax1.add_patch(circle)\n",
        "    ax1.text(x, y, label, ha='center', va='center', fontsize=9)\n",
        "\n",
        "# Connections\n",
        "ax1.annotate('', xy=(1.3, 1.5), xytext=(0.2, 1),\n",
        "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
        "ax1.annotate('', xy=(2.8, 1), xytext=(1.7, 1.5),\n",
        "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
        "ax1.annotate('', xy=(2.8, 1), xytext=(1.7, 0.5),\n",
        "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
        "ax1.annotate('', xy=(3.8, 1), xytext=(3.2, 1),\n",
        "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
        "\n",
        "ax1.text(2, -0.3, 'Evolvable: freq, amp, phase, biases', ha='center', fontsize=10)\n",
        "\n",
        "# Neural network\n",
        "ax2 = axes[1]\n",
        "ax2.set_xlim(-0.5, 4.5)\n",
        "ax2.set_ylim(-0.5, 2.5)\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Neural Network Mode\\n(Flexible network)', fontsize=12)\n",
        "\n",
        "# Input layer\n",
        "for i, label in enumerate(['dir', 'vel', 'dst', 't']):\n",
        "    y = 2 - i * 0.5\n",
        "    circle = plt.Circle((0, y), 0.15, color='lightblue', ec='blue', linewidth=2)\n",
        "    ax2.add_patch(circle)\n",
        "    ax2.text(0, y, label, ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Hidden layer\n",
        "for i in range(4):\n",
        "    y = 1.75 - i * 0.5\n",
        "    circle = plt.Circle((2, y), 0.15, color='lightyellow', ec='orange', linewidth=2)\n",
        "    ax2.add_patch(circle)\n",
        "\n",
        "# Output\n",
        "circle = plt.Circle((4, 1), 0.15, color='lightcoral', ec='red', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(4, 1, 'out', ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Connections (just a few to suggest fully connected)\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax2.plot([0.15, 1.85], [2 - i*0.5, 1.75 - j*0.5], 'gray', alpha=0.2, linewidth=0.5)\n",
        "for j in range(4):\n",
        "    ax2.plot([2.15, 3.85], [1.75 - j*0.5, 1], 'gray', alpha=0.2, linewidth=0.5)\n",
        "\n",
        "ax2.text(2, -0.3, 'Evolvable: ALL weights and biases', ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key insight: Oscillators are a highly constrained 'network' that's easy to evolve.\")\n",
        "print(\"Neural networks are more expressive but harder to optimize without gradients.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problem 4: Design a Hybrid Controller\n",
        "\n",
        "Implement a hybrid controller that combines oscillation with neural modulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | export\n",
        "class HybridController(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid controller: oscillator + neural modulation.\n",
        "    \n",
        "    Inputs:\n",
        "      - time: current simulation time\n",
        "      - sensors: [pellet_dir (3), velocity (3), distance (1), phase (1)]\n",
        "    \n",
        "    Output:\n",
        "      - muscle contractions with neural modulation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_muscles: int, hidden_size: int = 8):\n",
        "        super().__init__()\n",
        "        self.n_muscles = n_muscles\n",
        "        \n",
        "        # Oscillator parameters (learnable)\n",
        "        self.frequencies = nn.Parameter(torch.ones(n_muscles))\n",
        "        self.amplitudes = nn.Parameter(torch.ones(n_muscles) * 0.5)\n",
        "        self.phases = nn.Parameter(torch.zeros(n_muscles))\n",
        "        \n",
        "        # Neural modulation network\n",
        "        self.modulation_net = nn.Sequential(\n",
        "            nn.Linear(8, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, n_muscles),\n",
        "            nn.Tanh()  # Output in [-1, 1]\n",
        "        )\n",
        "        \n",
        "        self.modulation_strength = 0.5  # How much neural net can modulate\n",
        "    \n",
        "    def forward(self, time: torch.Tensor, sensors: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            time: scalar or (batch,) tensor\n",
        "            sensors: (batch, 8) sensor readings\n",
        "        \n",
        "        Returns:\n",
        "            (batch, n_muscles) muscle contractions\n",
        "        \"\"\"\n",
        "        # TODO: Implement hybrid control\n",
        "        # 1. Compute base oscillation for each muscle\n",
        "        # 2. Compute neural modulation\n",
        "        # 3. Combine them\n",
        "        \n",
        "        # fill in code here\n",
        "        \n",
        "        # Base oscillation\n",
        "        if time.dim() == 0:\n",
        "            time = time.unsqueeze(0)\n",
        "        base = self.amplitudes * torch.sin(\n",
        "            2 * np.pi * self.frequencies * time.unsqueeze(-1) + self.phases\n",
        "        )\n",
        "        \n",
        "        # Neural modulation\n",
        "        nn_out = self.modulation_net(sensors)\n",
        "        modulation = 1.0 + self.modulation_strength * nn_out\n",
        "        \n",
        "        # Combine\n",
        "        return base * modulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test hybrid controller\n",
        "set_seed(42)\n",
        "\n",
        "controller = HybridController(n_muscles=4)\n",
        "\n",
        "# Simulate a few timesteps\n",
        "times = torch.linspace(0, 2, 100)\n",
        "sensors = torch.randn(100, 8) * 0.1  # Small random sensor values\n",
        "\n",
        "contractions = []\n",
        "for t, s in zip(times, sensors):\n",
        "    c = controller(t, s.unsqueeze(0))\n",
        "    contractions.append(c.squeeze(0).detach())\n",
        "\n",
        "contractions = torch.stack(contractions)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "for i in range(4):\n",
        "    ax.plot(times.numpy(), contractions[:, i].numpy(), label=f'Muscle {i}')\n",
        "\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Contraction')\n",
        "ax.set_title('Hybrid Controller Output')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Verify output shape\n",
        "test_output = controller(torch.tensor(0.0), torch.randn(1, 8))\n",
        "assert test_output.shape == (1, 4), f\"Expected shape (1, 4), got {test_output.shape}\"\n",
        "print(\"Hybrid controller working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "1. **Gradient-free optimization** is necessary when:\n",
        "   - The loss function is non-differentiable\n",
        "   - Rewards are sparse\n",
        "   - The system has a black-box component (physics engine)\n",
        "\n",
        "2. **Genetic algorithms** provide a population-based search:\n",
        "   - Selection: survival of the fittest\n",
        "   - Crossover: combining good solutions\n",
        "   - Mutation: exploring nearby solutions\n",
        "\n",
        "3. **Neuroevolution** treats neural network weights as a genome:\n",
        "   - Flatten weights into a vector\n",
        "   - Apply GA operators\n",
        "   - Evaluate by running the network\n",
        "\n",
        "4. **NEAT** evolves network topology:\n",
        "   - Historical markings enable crossover\n",
        "   - Speciation protects innovation\n",
        "   - Starts minimal, grows as needed\n",
        "\n",
        "5. **Evolution Lab** uses these concepts:\n",
        "   - Oscillators: simple, fast to evolve\n",
        "   - Neural networks: flexible, slower to evolve\n",
        "   - Hybrid: best of both worlds\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **Gradients are great when available, but not always possible**\n",
        "- **Population-based search explores multiple solutions in parallel**\n",
        "- **Structure matters**: NEAT shows that evolving topology can be powerful\n",
        "- **Hybrid approaches often work best in practice**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Further Reading\n",
        "\n",
        "### Papers\n",
        "1. **NEAT (2002)**: Stanley & Miikkulainen - \"Evolving Neural Networks through Augmenting Topologies\"\n",
        "2. **Deep Neuroevolution (2017)**: Uber AI - Scaling GAs to deep networks\n",
        "3. **Weight Agnostic Neural Networks (2019)**: Gaier & Ha - Topology alone can encode behavior\n",
        "\n",
        "### Code\n",
        "- Evolution Lab: `/Users/silen/Desktop/Projects/genetic-algorithm`\n",
        "- NEAT-Python: `github.com/CodeReclaimers/neat-python`\n",
        "- PyTorch-NEAT: `github.com/uber-research/PyTorch-NEAT`\n",
        "\n",
        "### Books\n",
        "- \"Neuroevolution\" by Kenneth O. Stanley\n",
        "- \"Introduction to Evolutionary Computing\" by Eiben & Smith"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
